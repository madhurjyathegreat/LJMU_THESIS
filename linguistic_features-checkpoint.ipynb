{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466995a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1f0fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data=pd.read_csv(r'D:\\Downloads\\_bstractive_concatenated_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2f60a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=complete_data.iloc[y_test['Unnamed: 0.3'].tolist()]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a637f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed=pd.read_csv(r'D:\\Downloads\\X_test_preprocessed.csv')\n",
    "X_train_preprocessed=pd.read_csv(r'D:\\Downloads\\X_train_preprocessed.csv')\n",
    "X_train=pd.read_csv(r'D:\\Downloads\\X_train.csv')\n",
    "X_test=pd.read_csv(r'D:\\Downloads\\X_test.csv')\n",
    "y_train=pd.read_csv(r'D:\\Downloads\\y_train.csv')\n",
    "y_test=pd.read_csv(r'D:\\Downloads\\X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "5995ed60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "real    173\n",
       "fake    163\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48060e4f",
   "metadata": {},
   "source": [
    "### Linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3592e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Complexity Score = (Syllable Count + (Word Length / 3)) / (Word Frequency + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f58a491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyphen\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pyphen\n",
      "Successfully installed pyphen-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "171b14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens if re.sub(r'[^a-zA-Z0-9]', '', token)]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69a7b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_fake_genuine=[]\n",
    "for i in range(len(X_train)):\n",
    "    preprocessed_fake_genuine.append(preprocess_text(X_train['Fake_Genuine'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cae84070",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_fake_genuine_test=[]\n",
    "for i in range(len(X_test)):\n",
    "    preprocessed_fake_genuine_test.append(preprocess_text(X_test['Fake_Genuine'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "edc35720",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['preprocessed_fake_genuine']=preprocessed_fake_genuine\n",
    "X_test['preprocessed_fake_genuine_test']=preprocessed_fake_genuine_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1d3c94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "def count_syllables(word):\n",
    "    dic = pyphen.Pyphen(lang='en')  # Specify the language, e.g., 'en' for English\n",
    "    syllables = dic.inserted(word).count('-') + 1\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "186a1630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17.9 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###Complexity Score = (Syllable Count + (Word Length / 3)) / (Word Frequency + 1)\n",
    "complexity_words_count_per_sentence=[]\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "for index in range(len(X_train)):\n",
    "    complexity=[]\n",
    "    complexity_dict={}\n",
    "    for each_word in X_train['preprocessed_fake_genuine'].iloc[index].split(\" \"):\n",
    "        counter_dictionary=Counter(X_train['preprocessed_fake_genuine'].iloc[index].split(\" \"))\n",
    "        #print(each_word,count_syllables(each_word),len(each_word),counter_dictionary[each_word])\n",
    "        syllable_count=count_syllables(each_word)\n",
    "        word_length=len(each_word)\n",
    "        word_frequency=counter_dictionary[each_word]\n",
    "        complexity_score=(syllable_count+(word_length/3))/(word_frequency+1)\n",
    "        #print(\"Complexity_score\",complexity_score)\n",
    "        complexity.append(complexity_score)\n",
    "        complexity_dict.update({complexity_score:each_word})\n",
    "    threshold_complexity=np.percentile(complexity,80)\n",
    "    c=0\n",
    "    for i in complexity_dict.keys():\n",
    "        if i>=threshold_complexity:\n",
    "            c+=1\n",
    "    complexity_words_count_per_sentence.append(c)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8ea99d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complexity_words_count_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "823cef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['total_complex_words']=complexity_words_count_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "adf76db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.91 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###Complexity Score = (Syllable Count + (Word Length / 3)) / (Word Frequency + 1)\n",
    "complexity_words_count_per_sentence=[]\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "for index in range(len(X_test)):\n",
    "    complexity=[]\n",
    "    complexity_dict={}\n",
    "    for each_word in X_test['preprocessed_fake_genuine_test'].iloc[index].split(\" \"):\n",
    "        counter_dictionary=Counter(X_test['preprocessed_fake_genuine_test'].iloc[index].split(\" \"))\n",
    "        #print(each_word,count_syllables(each_word),len(each_word),counter_dictionary[each_word])\n",
    "        syllable_count=count_syllables(each_word)\n",
    "        word_length=len(each_word)\n",
    "        word_frequency=counter_dictionary[each_word]\n",
    "        complexity_score=(syllable_count+(word_length/3))/(word_frequency+1)\n",
    "        #print(\"Complexity_score\",complexity_score)\n",
    "        complexity.append(complexity_score)\n",
    "        complexity_dict.update({complexity_score:each_word})\n",
    "    threshold_complexity=np.percentile(complexity,80)\n",
    "    c=0\n",
    "    for i in complexity_dict.keys():\n",
    "        if i>=threshold_complexity:\n",
    "            c+=1\n",
    "    complexity_words_count_per_sentence.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c171711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['total_complex_words']=complexity_words_count_per_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be64c9",
   "metadata": {},
   "source": [
    "### 1.1 Gunning Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "05e520a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Craeting and engineered dataframe\n",
    "X_train_engineered=pd.DataFrame()\n",
    "X_train_engineered['total_complex_words']=X_train['total_complex_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4fb33620",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_engineered=pd.DataFrame()\n",
    "X_test_engineered['total_complex_words']=X_test['total_complex_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "35ab373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 85)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_engineered),len(X_test_engineered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2c2eb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to calculate gini fog index\n",
    "\n",
    "import nltk\n",
    "index=0\n",
    "gunning_trained=[]\n",
    "for sentence in X_train['Fake_Genuine']:\n",
    "    total_words = 0\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    sentences=nltk.sent_tokenize(sentence)\n",
    "    total_words += len(words)\n",
    "    #print(total_words)\n",
    "    #print(len(sentences))\n",
    "    complex_words=X_train['total_complex_words'][index]\n",
    "    percentage_complex_words = (complex_words / total_words) * 100\n",
    "    #print(percentage_complex_words)\n",
    "    avg_sentence_length = total_words / len(sentences)\n",
    "    gunning_fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    gunning_trained.append(gunning_fog_index)\n",
    "X_train_engineered['gunning_fog_index']=gunning_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d58578f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "gunning_test=[]\n",
    "for sentence in X_test['Fake_Genuine']:\n",
    "    total_words = 0\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    sentences=nltk.sent_tokenize(sentence)\n",
    "    total_words += len(words)\n",
    "    #print(total_words)\n",
    "    #print(len(sentences))\n",
    "    complex_words=X_test['total_complex_words'][index]\n",
    "    percentage_complex_words = (complex_words / total_words) * 100\n",
    "    #print(percentage_complex_words)\n",
    "    avg_sentence_length = total_words / len(sentences)\n",
    "    gunning_fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    gunning_test.append(gunning_fog_index)\n",
    "len(gunning_test)\n",
    "X_test_engineered['gunning_fog_index']=gunning_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154eb2e",
   "metadata": {},
   "source": [
    "### 1.2 Coleman_Liau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "795b9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coleman_liau(text):\n",
    "    import string\n",
    "    # Remove punctuation from the text\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the number of words, sentences, and characters\n",
    "    word_count = len(words)\n",
    "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    \n",
    "    # Calculate the average number of characters per 100 words\n",
    "    characters_per_100_words = (character_count / word_count) * 100\n",
    "    \n",
    "    # Calculate the average number of sentences per 100 words\n",
    "    sentences_per_100_words = (sentence_count / word_count) * 100\n",
    "    \n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau_index = (0.0588 * characters_per_100_words) - (0.296 * sentences_per_100_words) - 15.8\n",
    "    \n",
    "    return coleman_liau_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "16c178c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coleman_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    coleman_train.append(calculate_coleman_liau(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "14f056c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['coleman_liau']=coleman_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8b1ade27",
   "metadata": {},
   "outputs": [],
   "source": [
    "coleman_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    coleman_test.append(calculate_coleman_liau(text))\n",
    "X_test_engineered['coleman_liau']=coleman_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777de1d",
   "metadata": {},
   "source": [
    "### 1.3 Dale-Chall Readibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a8bc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "def generate_easy_words():\n",
    "    # Load the NLTK corpus of words\n",
    "    word_list = set(words.words())\n",
    "    \n",
    "    # Filter out words with more than 6 characters\n",
    "    easy_words = [word for word in word_list if len(word) <= 6]\n",
    "    \n",
    "    return easy_words\n",
    "\n",
    "# Example usage\n",
    "easy_words = generate_easy_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6b09beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_dale_chall(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Calculate the percentage of difficult words\n",
    "    difficult_words = [word for word in words if word.lower() not in easy_words]\n",
    "    percentage_difficult_words = (len(difficult_words) / len(words)) * 100\n",
    "    \n",
    "    # Calculate the Dale-Chall Readability score\n",
    "    raw_score = 0.1579 * (percentage_difficult_words) + 0.0496 * (len(words) / len(nltk.sent_tokenize(text)))\n",
    "    \n",
    "    # Adjust the score based on the average sentence length\n",
    "    if len(nltk.sent_tokenize(text)) > 0:\n",
    "        avg_sentence_length = len(words) / len(nltk.sent_tokenize(text))\n",
    "        raw_score += 0.5 if avg_sentence_length > 20 else 0\n",
    "    \n",
    "    return raw_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e5446509",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_dale_chall_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    calculate_dale_chall_train.append(calculate_dale_chall(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a878500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['dale_chall']=calculate_dale_chall_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ec47ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_dale_chall_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    calculate_dale_chall_test.append(calculate_dale_chall(text))\n",
    "X_test_engineered['dale_chall']=calculate_dale_chall_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "242a9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered.to_csv(r'D:\\Downloads\\checkpoint_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c8d06",
   "metadata": {},
   "source": [
    "### 1.4 Flesh Readibility Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1f152896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "def calculate_flesch_reading_ease(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Count the number of words, sentences, and syllables\n",
    "    num_words = len(words)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Estimate the number of syllables\n",
    "    dic = pyphen.Pyphen(lang='en_US')\n",
    "    num_syllables = 0\n",
    "    for word in words:\n",
    "        hyphenated_word = dic.inserted(word)\n",
    "        num_syllables += max(1, hyphenated_word.count('-') + 1)\n",
    "    \n",
    "    # Calculate the Flesch Reading Ease score\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    avg_syllables_per_word = num_syllables / num_words\n",
    "    \n",
    "    flesch_reading_ease = (\n",
    "        206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)\n",
    "    )\n",
    "    \n",
    "    return flesch_reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "35ea1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_flesch_reading_ease_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    calculate_flesch_reading_ease_train.append(calculate_flesch_reading_ease(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c15db1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['flesh_index']=calculate_flesch_reading_ease_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "434d0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_flesch_reading_ease_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    calculate_flesch_reading_ease_test.append(calculate_flesch_reading_ease(text))\n",
    "X_test_engineered['flesh_index']=calculate_flesch_reading_ease_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf583515",
   "metadata": {},
   "source": [
    "### 1.5 Spache Readibility Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "268ea856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spache(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Load the list of familiar words from the NLTK corpus\n",
    "    #familiar_words = set(nltk.corpus.words.words('spache'))\n",
    "    familiar_words=[]\n",
    "    dictionary_of_words=(Counter(words))\n",
    "    for keys in dictionary_of_words.keys():\n",
    "        if(dictionary_of_words[keys]>=4):\n",
    "            familiar_words.append(keys)\n",
    "    \n",
    "    # Calculate the percentage of familiar words\n",
    "    num_familiar_words = sum(1 for word in words if word in familiar_words)\n",
    "    percentage_familiar_words = (num_familiar_words / len(words)) * 100\n",
    "    \n",
    "    # Calculate the Spache Readability Index\n",
    "    spache_index = 0.141 * percentage_familiar_words + 0.086 * len(words)\n",
    "    \n",
    "    return spache_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3f6e8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_spache_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    calculate_spache_train.append(calculate_spache(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3aaf0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['spache']=calculate_spache_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2e006a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_spache_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    calculate_spache_test.append(calculate_spache(text))\n",
    "X_test_engineered['spache']=calculate_spache_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cddf16",
   "metadata": {},
   "source": [
    "### 1.6 Type_Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e469e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttr(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the number of unique words (types) and total words (tokens)\n",
    "    types = len(set(words))\n",
    "    tokens = len(words)\n",
    "    \n",
    "    # Calculate the Type-Token Ratio\n",
    "    ttr = types / tokens\n",
    "    \n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "18fecaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    ttr_train.append(calculate_ttr(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c6802bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['ttr']=ttr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3a74fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    ttr_test.append(calculate_ttr(text))\n",
    "X_test_engineered['ttr']=ttr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e45a4",
   "metadata": {},
   "source": [
    "### 1.7 Log-Type-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2c835da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log\n",
    "\n",
    "def calculate_log_ttr(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the number of word types and word tokens\n",
    "    word_types = set(words)\n",
    "    word_tokens = len(words)\n",
    "\n",
    "    # Calculate the logTTR\n",
    "    log_ttr = log(len(word_types)) / log(word_tokens)\n",
    "\n",
    "    return log_ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1026fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lttr_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    lttr_train.append(calculate_log_ttr(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9f8f28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['lttr']=lttr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "045bbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lttr_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    lttr_test.append(calculate_log_ttr(text))\n",
    "X_test_engineered['lttr']=lttr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778d1ba",
   "metadata": {},
   "source": [
    "### 1.8 Guiraud's Root TTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c88e5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_guirauds_root_ttr(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the number of unique words (types) and total words (tokens)\n",
    "    types = len(set(words))\n",
    "    tokens = len(words)\n",
    "    \n",
    "    # Calculate the Type-Token Ratio\n",
    "    grttr = types / math.sqrt(tokens)    \n",
    "    return grttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ccd4a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "grttr_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    grttr_train.append(calculate_guirauds_root_ttr(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6970470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['grttr']=grttr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fd099bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grttr_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    grttr_test.append(calculate_guirauds_root_ttr(text))\n",
    "X_test_engineered['grttr']=grttr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1abdea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_complex_words</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>flesh_index</th>\n",
       "      <th>spache</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lttr</th>\n",
       "      <th>grttr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13.958368</td>\n",
       "      <td>16.042899</td>\n",
       "      <td>13.194432</td>\n",
       "      <td>45.187189</td>\n",
       "      <td>25.627640</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.894344</td>\n",
       "      <td>9.313644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11.743640</td>\n",
       "      <td>12.374202</td>\n",
       "      <td>10.910638</td>\n",
       "      <td>66.336336</td>\n",
       "      <td>36.455923</td>\n",
       "      <td>0.665584</td>\n",
       "      <td>0.898881</td>\n",
       "      <td>11.680959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>12.725641</td>\n",
       "      <td>12.238621</td>\n",
       "      <td>11.165187</td>\n",
       "      <td>55.850096</td>\n",
       "      <td>47.448641</td>\n",
       "      <td>0.529557</td>\n",
       "      <td>0.865746</td>\n",
       "      <td>10.670271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>12.945318</td>\n",
       "      <td>14.739058</td>\n",
       "      <td>12.977291</td>\n",
       "      <td>47.256816</td>\n",
       "      <td>45.093900</td>\n",
       "      <td>0.568063</td>\n",
       "      <td>0.878406</td>\n",
       "      <td>11.102686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>15.092105</td>\n",
       "      <td>14.640398</td>\n",
       "      <td>12.863592</td>\n",
       "      <td>39.916678</td>\n",
       "      <td>54.833158</td>\n",
       "      <td>0.602386</td>\n",
       "      <td>0.895130</td>\n",
       "      <td>13.510102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_complex_words  gunning_fog_index  coleman_liau  dale_chall  \\\n",
       "0                   12          13.958368     16.042899   13.194432   \n",
       "1                   11          11.743640     12.374202   10.910638   \n",
       "2                   14          12.725641     12.238621   11.165187   \n",
       "3                   16          12.945318     14.739058   12.977291   \n",
       "4                   16          15.092105     14.640398   12.863592   \n",
       "\n",
       "   flesh_index     spache       ttr      lttr      grttr  \n",
       "0    45.187189  25.627640  0.647343  0.894344   9.313644  \n",
       "1    66.336336  36.455923  0.665584  0.898881  11.680959  \n",
       "2    55.850096  47.448641  0.529557  0.865746  10.670271  \n",
       "3    47.256816  45.093900  0.568063  0.878406  11.102686  \n",
       "4    39.916678  54.833158  0.602386  0.895130  13.510102  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57b1bb",
   "metadata": {},
   "source": [
    "### 1.9 Caroll’s Corrected TTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6211cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_carolls_root_ttr(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the number of unique words (types) and total words (tokens)\n",
    "    types = len(set(words))\n",
    "    tokens = len(words)\n",
    "    \n",
    "    # Calculate the Type-Token Ratio\n",
    "    cttr = types / (1.41*tokens)   \n",
    "    return cttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d50faeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cttr_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    cttr_train.append(calculate_carolls_root_ttr(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5381fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['cttr']=cttr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "71741ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cttr_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    cttr_test.append(calculate_carolls_root_ttr(text))\n",
    "X_test_engineered['cttr']=cttr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafebf45",
   "metadata": {},
   "source": [
    "### 1.10 Dugasts Uber Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "db3160b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dugasts_uber_index(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the number of unique words (types) and total words (tokens)\n",
    "    types = len(set(words))\n",
    "    tokens = len(words)\n",
    "    \n",
    "    # Calculate the Type-Token Ratio\n",
    "    dui = (math.log(tokens)**2)/(math.log(tokens)-math.log(types))\n",
    "    return dui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d363f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dui_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    dui_train.append(dugasts_uber_index(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f1f76e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['dui']=dui_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f5ecc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dui_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    dui_test.append(dugasts_uber_index(text))\n",
    "X_test_engineered['dui']=dui_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595161b",
   "metadata": {},
   "source": [
    "### 1.11 Summer's Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dee88dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summers_index(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the number of unique words (types) and total words (tokens)\n",
    "    types = len(set(words))\n",
    "    tokens = len(words)\n",
    "    \n",
    "    # Calculate the Type-Token Ratio\n",
    "    si = math.log(math.log(types))/math.log(math.log(tokens))\n",
    "    return si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "19610ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    si_train.append(summers_index(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c66d41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['si']=si_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "19e08415",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    si_test.append(summers_index(text))\n",
    "X_test_engineered['si']=si_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992965d3",
   "metadata": {},
   "source": [
    "### 1.12 Yule's K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0ed1af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    tokens = re.split(r\"[^0-9A-Za-z\\-'_]+\", s)\n",
    "    return tokens\n",
    "def get_yules(s):\n",
    "    from collections import Counter\n",
    "    tokens = tokenize(s)\n",
    "    token_counter = Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1f20890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yulesk_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    yulesk_train.append(get_yules(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "37e42678",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['yulesk']=yulesk_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5f9af21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yulesk_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    yulesk_test.append(get_yules(text))\n",
    "X_test_engineered['yulesk']=yulesk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d1838",
   "metadata": {},
   "source": [
    "### 1.13 Yule's I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e61d12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    tokens = re.split(r\"[^0-9A-Za-z\\-'_]+\", s)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2b7738a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yules(s):\n",
    "    from collections import Counter\n",
    "    tokens = tokenize(s)\n",
    "    token_counter = Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "10d6327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yulesi_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    yulesi_train.append(get_yules(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "01d1de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['yulesi']=yulesi_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8dc5b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "yulesi_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    yulesi_test.append(get_yules(text))\n",
    "X_test_engineered['yulesi']=yulesi_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66422c2",
   "metadata": {},
   "source": [
    "### 1.14 Simpsons D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "964ae94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "def calculate_simpsons_d(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Calculate the frequency of each word\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    # Calculate the proportion of each word\n",
    "    word_proportions = {word: count / len(words) for word, count in word_freq.items()}\n",
    "\n",
    "    # Calculate Simpson's D\n",
    "    simpsons_d = sum(proportion * (proportion - 1) for proportion in word_proportions.values())\n",
    "\n",
    "    return 1 - simpsons_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ed62a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsonsd_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    simpsonsd_train.append(calculate_simpsons_d(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "562135f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['simpsonsd']=simpsonsd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d4a0d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsonsd_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    simpsonsd_test.append(calculate_simpsons_d(text))\n",
    "X_test_engineered['simpsonsd']=simpsonsd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1c919",
   "metadata": {},
   "source": [
    "### 1.15 Herdan's Vm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c64afea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_herdans_vm(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Calculate the total number of words\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Calculate the number of distinct words\n",
    "    unique_words = len(set(words))\n",
    "    \n",
    "    # Calculate Herdan's VM\n",
    "    herdans_vm = unique_words / (total_words ** 0.5)\n",
    "    \n",
    "    return herdans_vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "434aa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "herdans_vm_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    herdans_vm_train.append(calculate_herdans_vm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7874b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['herdans']=herdans_vm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b0b02314",
   "metadata": {},
   "outputs": [],
   "source": [
    "herdans_vm_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    herdans_vm_test.append(calculate_herdans_vm(text))\n",
    "X_test_engineered['herdans']=herdans_vm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9e15dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered.to_csv(r\"D:\\Downloads\\complete_linguistic_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "77e8c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_engineered.to_csv(r\"D:\\Downloads\\complete_linguistic_features_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0f6d6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(r'D:\\Downloads\\X_train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5e56d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(r'D:\\Downloads\\X_test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424779ba",
   "metadata": {},
   "source": [
    "### Incoporating mmf clusters in the enginnered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "806ea106",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmf_clusters_train=pd.read_csv(r'D:\\Downloads\\mmf_clusters_train.csv')\n",
    "mmf_clusters_test=pd.read_csv(r'D:\\Downloads\\mmf_clusters_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cefd9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['mmf_clusters']=mmf_clusters_train['mmf_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fc2bbf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_engineered['mmf_clusters']=mmf_clusters_test['mmf_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bfac1633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_complex_words</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>flesh_index</th>\n",
       "      <th>spache</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lttr</th>\n",
       "      <th>grttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>dui</th>\n",
       "      <th>si</th>\n",
       "      <th>yulesk</th>\n",
       "      <th>yulesi</th>\n",
       "      <th>simpsonsd</th>\n",
       "      <th>herdans</th>\n",
       "      <th>mmf_clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>10.827839</td>\n",
       "      <td>11.008814</td>\n",
       "      <td>9.651423</td>\n",
       "      <td>61.472051</td>\n",
       "      <td>50.936890</td>\n",
       "      <td>0.588983</td>\n",
       "      <td>0.895790</td>\n",
       "      <td>12.795987</td>\n",
       "      <td>0.417718</td>\n",
       "      <td>71.612027</td>\n",
       "      <td>0.950539</td>\n",
       "      <td>95.950145</td>\n",
       "      <td>104.220791</td>\n",
       "      <td>1.988191</td>\n",
       "      <td>11.237153</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>15.682480</td>\n",
       "      <td>14.623481</td>\n",
       "      <td>12.002560</td>\n",
       "      <td>39.317823</td>\n",
       "      <td>32.517072</td>\n",
       "      <td>0.604096</td>\n",
       "      <td>0.902660</td>\n",
       "      <td>10.340450</td>\n",
       "      <td>0.428437</td>\n",
       "      <td>64.013683</td>\n",
       "      <td>0.946505</td>\n",
       "      <td>88.851867</td>\n",
       "      <td>112.546875</td>\n",
       "      <td>1.987864</td>\n",
       "      <td>9.686140</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>10.795483</td>\n",
       "      <td>10.879403</td>\n",
       "      <td>11.212054</td>\n",
       "      <td>71.349718</td>\n",
       "      <td>63.229321</td>\n",
       "      <td>0.600746</td>\n",
       "      <td>0.884849</td>\n",
       "      <td>13.908282</td>\n",
       "      <td>0.426061</td>\n",
       "      <td>77.495466</td>\n",
       "      <td>0.953990</td>\n",
       "      <td>73.753946</td>\n",
       "      <td>135.585965</td>\n",
       "      <td>1.987768</td>\n",
       "      <td>11.300794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>12.966712</td>\n",
       "      <td>11.955512</td>\n",
       "      <td>10.786522</td>\n",
       "      <td>55.993690</td>\n",
       "      <td>65.495157</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.876743</td>\n",
       "      <td>12.500406</td>\n",
       "      <td>0.357493</td>\n",
       "      <td>60.195948</td>\n",
       "      <td>0.939339</td>\n",
       "      <td>108.518351</td>\n",
       "      <td>92.150313</td>\n",
       "      <td>1.986678</td>\n",
       "      <td>11.054827</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>8.020596</td>\n",
       "      <td>11.693594</td>\n",
       "      <td>10.947900</td>\n",
       "      <td>77.559871</td>\n",
       "      <td>63.766915</td>\n",
       "      <td>0.650391</td>\n",
       "      <td>0.883435</td>\n",
       "      <td>14.716660</td>\n",
       "      <td>0.461270</td>\n",
       "      <td>90.465621</td>\n",
       "      <td>0.960971</td>\n",
       "      <td>63.219625</td>\n",
       "      <td>158.178731</td>\n",
       "      <td>1.987387</td>\n",
       "      <td>10.932163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_complex_words  gunning_fog_index  coleman_liau  dale_chall  \\\n",
       "0                   12          10.827839     11.008814    9.651423   \n",
       "1                   14          15.682480     14.623481   12.002560   \n",
       "2                   16          10.795483     10.879403   11.212054   \n",
       "3                   17          12.966712     11.955512   10.786522   \n",
       "4                   11           8.020596     11.693594   10.947900   \n",
       "\n",
       "   flesh_index     spache       ttr      lttr      grttr      cttr        dui  \\\n",
       "0    61.472051  50.936890  0.588983  0.895790  12.795987  0.417718  71.612027   \n",
       "1    39.317823  32.517072  0.604096  0.902660  10.340450  0.428437  64.013683   \n",
       "2    71.349718  63.229321  0.600746  0.884849  13.908282  0.426061  77.495466   \n",
       "3    55.993690  65.495157  0.504065  0.876743  12.500406  0.357493  60.195948   \n",
       "4    77.559871  63.766915  0.650391  0.883435  14.716660  0.461270  90.465621   \n",
       "\n",
       "         si      yulesk      yulesi  simpsonsd    herdans  mmf_clusters  \n",
       "0  0.950539   95.950145  104.220791   1.988191  11.237153             3  \n",
       "1  0.946505   88.851867  112.546875   1.987864   9.686140            -1  \n",
       "2  0.953990   73.753946  135.585965   1.987768  11.300794             0  \n",
       "3  0.939339  108.518351   92.150313   1.986678  11.054827             0  \n",
       "4  0.960971   63.219625  158.178731   1.987387  10.932163             1  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_engineered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "03cced81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_complex_words</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>flesh_index</th>\n",
       "      <th>spache</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lttr</th>\n",
       "      <th>grttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>dui</th>\n",
       "      <th>si</th>\n",
       "      <th>yulesk</th>\n",
       "      <th>yulesi</th>\n",
       "      <th>simpsonsd</th>\n",
       "      <th>herdans</th>\n",
       "      <th>mmf_clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13.958368</td>\n",
       "      <td>16.042899</td>\n",
       "      <td>13.194432</td>\n",
       "      <td>45.187189</td>\n",
       "      <td>25.627640</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.894344</td>\n",
       "      <td>9.313644</td>\n",
       "      <td>0.459109</td>\n",
       "      <td>65.392650</td>\n",
       "      <td>0.949179</td>\n",
       "      <td>87.370562</td>\n",
       "      <td>114.455026</td>\n",
       "      <td>1.985697</td>\n",
       "      <td>8.409001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11.743640</td>\n",
       "      <td>12.374202</td>\n",
       "      <td>10.910638</td>\n",
       "      <td>66.336336</td>\n",
       "      <td>36.455923</td>\n",
       "      <td>0.665584</td>\n",
       "      <td>0.898881</td>\n",
       "      <td>11.680959</td>\n",
       "      <td>0.472046</td>\n",
       "      <td>80.655529</td>\n",
       "      <td>0.957786</td>\n",
       "      <td>92.069940</td>\n",
       "      <td>108.613082</td>\n",
       "      <td>1.988271</td>\n",
       "      <td>9.801463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>12.725641</td>\n",
       "      <td>12.238621</td>\n",
       "      <td>11.165187</td>\n",
       "      <td>55.850096</td>\n",
       "      <td>47.448641</td>\n",
       "      <td>0.529557</td>\n",
       "      <td>0.865746</td>\n",
       "      <td>10.670271</td>\n",
       "      <td>0.375572</td>\n",
       "      <td>56.749126</td>\n",
       "      <td>0.937600</td>\n",
       "      <td>228.411962</td>\n",
       "      <td>43.780544</td>\n",
       "      <td>1.975126</td>\n",
       "      <td>8.828978</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>12.945318</td>\n",
       "      <td>14.739058</td>\n",
       "      <td>12.977291</td>\n",
       "      <td>47.256816</td>\n",
       "      <td>45.093900</td>\n",
       "      <td>0.568063</td>\n",
       "      <td>0.878406</td>\n",
       "      <td>11.102686</td>\n",
       "      <td>0.402881</td>\n",
       "      <td>62.504991</td>\n",
       "      <td>0.943930</td>\n",
       "      <td>123.017954</td>\n",
       "      <td>81.288947</td>\n",
       "      <td>1.986088</td>\n",
       "      <td>9.513030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>15.092105</td>\n",
       "      <td>14.640398</td>\n",
       "      <td>12.863592</td>\n",
       "      <td>39.916678</td>\n",
       "      <td>54.833158</td>\n",
       "      <td>0.602386</td>\n",
       "      <td>0.895130</td>\n",
       "      <td>13.510102</td>\n",
       "      <td>0.427224</td>\n",
       "      <td>76.344441</td>\n",
       "      <td>0.953502</td>\n",
       "      <td>102.403620</td>\n",
       "      <td>97.652798</td>\n",
       "      <td>1.987356</td>\n",
       "      <td>11.476597</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_complex_words  gunning_fog_index  coleman_liau  dale_chall  \\\n",
       "0                   12          13.958368     16.042899   13.194432   \n",
       "1                   11          11.743640     12.374202   10.910638   \n",
       "2                   14          12.725641     12.238621   11.165187   \n",
       "3                   16          12.945318     14.739058   12.977291   \n",
       "4                   16          15.092105     14.640398   12.863592   \n",
       "\n",
       "   flesh_index     spache       ttr      lttr      grttr      cttr        dui  \\\n",
       "0    45.187189  25.627640  0.647343  0.894344   9.313644  0.459109  65.392650   \n",
       "1    66.336336  36.455923  0.665584  0.898881  11.680959  0.472046  80.655529   \n",
       "2    55.850096  47.448641  0.529557  0.865746  10.670271  0.375572  56.749126   \n",
       "3    47.256816  45.093900  0.568063  0.878406  11.102686  0.402881  62.504991   \n",
       "4    39.916678  54.833158  0.602386  0.895130  13.510102  0.427224  76.344441   \n",
       "\n",
       "         si      yulesk      yulesi  simpsonsd    herdans  mmf_clusters  \n",
       "0  0.949179   87.370562  114.455026   1.985697   8.409001             0  \n",
       "1  0.957786   92.069940  108.613082   1.988271   9.801463             0  \n",
       "2  0.937600  228.411962   43.780544   1.975126   8.828978             3  \n",
       "3  0.943930  123.017954   81.288947   1.986088   9.513030             0  \n",
       "4  0.953502  102.403620   97.652798   1.987356  11.476597             3  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32713d6c",
   "metadata": {},
   "source": [
    "### Implementing RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "421ff06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_model = RandomForestClassifier(random_state=12)\n",
    "\n",
    "# Define the parameter grid for the random search\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(100,250,10),\n",
    "    'max_depth': np.arange(1, 10),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "08bed991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=12),\n",
       "                   param_distributions={&#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=12),\n",
       "                   param_distributions={&#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=12)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=12)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=12),\n",
       "                   param_distributions={'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'n_estimators': array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])})"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(rf_model, param_grid, n_iter=10, cv=5)\n",
    "random_search.fit(X_train_engineered, y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1922da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 230, 'min_samples_split': 6, 'min_samples_leaf': 9, 'max_depth': 3}\n",
      "Best Score: 0.46413520632133454\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966183e",
   "metadata": {},
   "source": [
    "### Implementing More Features in the Engineered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599afe8",
   "metadata": {},
   "source": [
    "### Syntactic Features\n",
    "### S1.1 No of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6033fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_words=[]\n",
    "for i in X_train['Fake_Genuine']:\n",
    "    no_of_words.append(len(i.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d1c9c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_words_test=[]\n",
    "for i in X_test['Fake_Genuine']:\n",
    "    no_of_words_test.append(len(i.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203c9b8",
   "metadata": {},
   "source": [
    "### S1.2 No of upper case Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ed2f2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_upper=[]\n",
    "for i in X_train['Fake_Genuine']:\n",
    "    c=0\n",
    "    words=i.split(\" \")\n",
    "    for j in words:\n",
    "        if j[0].isupper():\n",
    "            c+=1\n",
    "    no_of_upper.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cbccdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_upper_test=[]\n",
    "for i in X_test['Fake_Genuine']:\n",
    "    c=0\n",
    "    words=i.split(\" \")\n",
    "    for j in words:\n",
    "        if j[0].isupper():\n",
    "            c+=1\n",
    "    no_of_upper_test.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ba06a",
   "metadata": {},
   "source": [
    "### S1.3 No of lower case Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1a320b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_lower=[]\n",
    "for i in X_train['Fake_Genuine']:\n",
    "    c=0\n",
    "    words=i.split(\" \")\n",
    "    for j in words:\n",
    "        if j[0].isupper()==False:\n",
    "            c+=1\n",
    "    no_of_lower.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b2941a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_lower_test=[]\n",
    "for i in X_test['Fake_Genuine']:\n",
    "    c=0\n",
    "    words=i.split(\" \")\n",
    "    for j in words:\n",
    "        if j[0].isupper()==False:\n",
    "            c+=1\n",
    "    no_of_lower_test.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5ae34",
   "metadata": {},
   "source": [
    "### S1.4 No of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ca7e908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_sentences=[]\n",
    "for i in X_train['Fake_Genuine']:\n",
    "    no_of_sentences.append(len(nltk.sent_tokenize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3c3750af",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_sentences_test=[]\n",
    "for i in X_test['Fake_Genuine']:\n",
    "    no_of_sentences_test.append(len(nltk.sent_tokenize(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfe768",
   "metadata": {},
   "source": [
    "### S1.5 No of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8ba61b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_syllables=[]\n",
    "for i in X_train['Fake_Genuine']:\n",
    "    c=0\n",
    "    words=i.split(\" \")\n",
    "    for j in words:\n",
    "        c=c+count_syllables(j)\n",
    "    no_of_syllables.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "aed38d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_syllables_test=[]\n",
    "for i in X_test['Fake_Genuine']:\n",
    "    c=0\n",
    "    words=i.split(\" \")\n",
    "    for j in words:\n",
    "        c=c+count_syllables(j)\n",
    "    no_of_syllables_test.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfb305",
   "metadata": {},
   "source": [
    "### S1.6 No of special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ee7431d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def count_special_characters(text):\n",
    "    special_chars = set(string.punctuation)  # Set of special characters\n",
    "    \n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if char in special_chars:\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bfe02451",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters_train=[]\n",
    "for text in X_train['Fake_Genuine']:\n",
    "    special_characters_train.append(count_special_characters(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "63f58480",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters_test=[]\n",
    "for text in X_test['Fake_Genuine']:\n",
    "    special_characters_test.append(count_special_characters(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "272661f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['words']=no_of_words\n",
    "X_train_engineered['upper']=no_of_upper\n",
    "X_train_engineered['lower']=no_of_lower\n",
    "X_train_engineered['sentences']=no_of_sentences\n",
    "X_train_engineered['syllables']=no_of_syllables\n",
    "X_train_engineered['special_characters']=special_characters_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7eff7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_engineered['words']=no_of_words_test\n",
    "X_test_engineered['upper']=no_of_upper_test\n",
    "X_test_engineered['lower']=no_of_lower_test\n",
    "X_test_engineered['sentences']=no_of_sentences_test\n",
    "X_test_engineered['syllables']=no_of_syllables_test\n",
    "X_test_engineered['special_characters']=special_characters_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "89d85780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_complex_words</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>flesh_index</th>\n",
       "      <th>spache</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lttr</th>\n",
       "      <th>grttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>dui</th>\n",
       "      <th>si</th>\n",
       "      <th>yulesk</th>\n",
       "      <th>yulesi</th>\n",
       "      <th>simpsonsd</th>\n",
       "      <th>herdans</th>\n",
       "      <th>mmf_clusters</th>\n",
       "      <th>words</th>\n",
       "      <th>upper</th>\n",
       "      <th>lower</th>\n",
       "      <th>sentences</th>\n",
       "      <th>syllables</th>\n",
       "      <th>special_characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>10.827839</td>\n",
       "      <td>11.008814</td>\n",
       "      <td>9.651423</td>\n",
       "      <td>61.472051</td>\n",
       "      <td>50.936890</td>\n",
       "      <td>0.588983</td>\n",
       "      <td>0.895790</td>\n",
       "      <td>12.795987</td>\n",
       "      <td>0.417718</td>\n",
       "      <td>71.612027</td>\n",
       "      <td>0.950539</td>\n",
       "      <td>95.950145</td>\n",
       "      <td>104.220791</td>\n",
       "      <td>1.988191</td>\n",
       "      <td>11.237153</td>\n",
       "      <td>3</td>\n",
       "      <td>460</td>\n",
       "      <td>51</td>\n",
       "      <td>409</td>\n",
       "      <td>21</td>\n",
       "      <td>643</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>15.682480</td>\n",
       "      <td>14.623481</td>\n",
       "      <td>12.002560</td>\n",
       "      <td>39.317823</td>\n",
       "      <td>32.517072</td>\n",
       "      <td>0.604096</td>\n",
       "      <td>0.902660</td>\n",
       "      <td>10.340450</td>\n",
       "      <td>0.428437</td>\n",
       "      <td>64.013683</td>\n",
       "      <td>0.946505</td>\n",
       "      <td>88.851867</td>\n",
       "      <td>112.546875</td>\n",
       "      <td>1.987864</td>\n",
       "      <td>9.686140</td>\n",
       "      <td>-1</td>\n",
       "      <td>286</td>\n",
       "      <td>49</td>\n",
       "      <td>237</td>\n",
       "      <td>9</td>\n",
       "      <td>448</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>10.795483</td>\n",
       "      <td>10.879403</td>\n",
       "      <td>11.212054</td>\n",
       "      <td>71.349718</td>\n",
       "      <td>63.229321</td>\n",
       "      <td>0.600746</td>\n",
       "      <td>0.884849</td>\n",
       "      <td>13.908282</td>\n",
       "      <td>0.426061</td>\n",
       "      <td>77.495466</td>\n",
       "      <td>0.953990</td>\n",
       "      <td>73.753946</td>\n",
       "      <td>135.585965</td>\n",
       "      <td>1.987768</td>\n",
       "      <td>11.300794</td>\n",
       "      <td>0</td>\n",
       "      <td>528</td>\n",
       "      <td>76</td>\n",
       "      <td>452</td>\n",
       "      <td>26</td>\n",
       "      <td>702</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>12.966712</td>\n",
       "      <td>11.955512</td>\n",
       "      <td>10.786522</td>\n",
       "      <td>55.993690</td>\n",
       "      <td>65.495157</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.876743</td>\n",
       "      <td>12.500406</td>\n",
       "      <td>0.357493</td>\n",
       "      <td>60.195948</td>\n",
       "      <td>0.939339</td>\n",
       "      <td>108.518351</td>\n",
       "      <td>92.150313</td>\n",
       "      <td>1.986678</td>\n",
       "      <td>11.054827</td>\n",
       "      <td>0</td>\n",
       "      <td>605</td>\n",
       "      <td>81</td>\n",
       "      <td>524</td>\n",
       "      <td>22</td>\n",
       "      <td>857</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>8.020596</td>\n",
       "      <td>11.693594</td>\n",
       "      <td>10.947900</td>\n",
       "      <td>77.559871</td>\n",
       "      <td>63.766915</td>\n",
       "      <td>0.650391</td>\n",
       "      <td>0.883435</td>\n",
       "      <td>14.716660</td>\n",
       "      <td>0.461270</td>\n",
       "      <td>90.465621</td>\n",
       "      <td>0.960971</td>\n",
       "      <td>63.219625</td>\n",
       "      <td>158.178731</td>\n",
       "      <td>1.987387</td>\n",
       "      <td>10.932163</td>\n",
       "      <td>1</td>\n",
       "      <td>503</td>\n",
       "      <td>98</td>\n",
       "      <td>405</td>\n",
       "      <td>36</td>\n",
       "      <td>686</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_complex_words  gunning_fog_index  coleman_liau  dale_chall  \\\n",
       "0                   12          10.827839     11.008814    9.651423   \n",
       "1                   14          15.682480     14.623481   12.002560   \n",
       "2                   16          10.795483     10.879403   11.212054   \n",
       "3                   17          12.966712     11.955512   10.786522   \n",
       "4                   11           8.020596     11.693594   10.947900   \n",
       "\n",
       "   flesh_index     spache       ttr      lttr      grttr      cttr        dui  \\\n",
       "0    61.472051  50.936890  0.588983  0.895790  12.795987  0.417718  71.612027   \n",
       "1    39.317823  32.517072  0.604096  0.902660  10.340450  0.428437  64.013683   \n",
       "2    71.349718  63.229321  0.600746  0.884849  13.908282  0.426061  77.495466   \n",
       "3    55.993690  65.495157  0.504065  0.876743  12.500406  0.357493  60.195948   \n",
       "4    77.559871  63.766915  0.650391  0.883435  14.716660  0.461270  90.465621   \n",
       "\n",
       "         si      yulesk      yulesi  simpsonsd    herdans  mmf_clusters  \\\n",
       "0  0.950539   95.950145  104.220791   1.988191  11.237153             3   \n",
       "1  0.946505   88.851867  112.546875   1.987864   9.686140            -1   \n",
       "2  0.953990   73.753946  135.585965   1.987768  11.300794             0   \n",
       "3  0.939339  108.518351   92.150313   1.986678  11.054827             0   \n",
       "4  0.960971   63.219625  158.178731   1.987387  10.932163             1   \n",
       "\n",
       "   words  upper  lower  sentences  syllables  special_characters  \n",
       "0    460     51    409         21        643                  54  \n",
       "1    286     49    237          9        448                  34  \n",
       "2    528     76    452         26        702                  80  \n",
       "3    605     81    524         22        857                  54  \n",
       "4    503     98    405         36        686                  91  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "X_train_engineered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f86c09ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_complex_words</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>flesh_index</th>\n",
       "      <th>spache</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lttr</th>\n",
       "      <th>grttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>dui</th>\n",
       "      <th>si</th>\n",
       "      <th>yulesk</th>\n",
       "      <th>yulesi</th>\n",
       "      <th>simpsonsd</th>\n",
       "      <th>herdans</th>\n",
       "      <th>mmf_clusters</th>\n",
       "      <th>words</th>\n",
       "      <th>upper</th>\n",
       "      <th>lower</th>\n",
       "      <th>sentences</th>\n",
       "      <th>syllables</th>\n",
       "      <th>special_characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13.958368</td>\n",
       "      <td>16.042899</td>\n",
       "      <td>13.194432</td>\n",
       "      <td>45.187189</td>\n",
       "      <td>25.627640</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.894344</td>\n",
       "      <td>9.313644</td>\n",
       "      <td>0.459109</td>\n",
       "      <td>65.392650</td>\n",
       "      <td>0.949179</td>\n",
       "      <td>87.370562</td>\n",
       "      <td>114.455026</td>\n",
       "      <td>1.985697</td>\n",
       "      <td>8.409001</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>47</td>\n",
       "      <td>154</td>\n",
       "      <td>8</td>\n",
       "      <td>318</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11.743640</td>\n",
       "      <td>12.374202</td>\n",
       "      <td>10.910638</td>\n",
       "      <td>66.336336</td>\n",
       "      <td>36.455923</td>\n",
       "      <td>0.665584</td>\n",
       "      <td>0.898881</td>\n",
       "      <td>11.680959</td>\n",
       "      <td>0.472046</td>\n",
       "      <td>80.655529</td>\n",
       "      <td>0.957786</td>\n",
       "      <td>92.069940</td>\n",
       "      <td>108.613082</td>\n",
       "      <td>1.988271</td>\n",
       "      <td>9.801463</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>50</td>\n",
       "      <td>249</td>\n",
       "      <td>14</td>\n",
       "      <td>414</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>12.725641</td>\n",
       "      <td>12.238621</td>\n",
       "      <td>11.165187</td>\n",
       "      <td>55.850096</td>\n",
       "      <td>47.448641</td>\n",
       "      <td>0.529557</td>\n",
       "      <td>0.865746</td>\n",
       "      <td>10.670271</td>\n",
       "      <td>0.375572</td>\n",
       "      <td>56.749126</td>\n",
       "      <td>0.937600</td>\n",
       "      <td>228.411962</td>\n",
       "      <td>43.780544</td>\n",
       "      <td>1.975126</td>\n",
       "      <td>8.828978</td>\n",
       "      <td>3</td>\n",
       "      <td>397</td>\n",
       "      <td>70</td>\n",
       "      <td>327</td>\n",
       "      <td>16</td>\n",
       "      <td>574</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>12.945318</td>\n",
       "      <td>14.739058</td>\n",
       "      <td>12.977291</td>\n",
       "      <td>47.256816</td>\n",
       "      <td>45.093900</td>\n",
       "      <td>0.568063</td>\n",
       "      <td>0.878406</td>\n",
       "      <td>11.102686</td>\n",
       "      <td>0.402881</td>\n",
       "      <td>62.504991</td>\n",
       "      <td>0.943930</td>\n",
       "      <td>123.017954</td>\n",
       "      <td>81.288947</td>\n",
       "      <td>1.986088</td>\n",
       "      <td>9.513030</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>51</td>\n",
       "      <td>326</td>\n",
       "      <td>15</td>\n",
       "      <td>590</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>15.092105</td>\n",
       "      <td>14.640398</td>\n",
       "      <td>12.863592</td>\n",
       "      <td>39.916678</td>\n",
       "      <td>54.833158</td>\n",
       "      <td>0.602386</td>\n",
       "      <td>0.895130</td>\n",
       "      <td>13.510102</td>\n",
       "      <td>0.427224</td>\n",
       "      <td>76.344441</td>\n",
       "      <td>0.953502</td>\n",
       "      <td>102.403620</td>\n",
       "      <td>97.652798</td>\n",
       "      <td>1.987356</td>\n",
       "      <td>11.476597</td>\n",
       "      <td>3</td>\n",
       "      <td>491</td>\n",
       "      <td>60</td>\n",
       "      <td>431</td>\n",
       "      <td>16</td>\n",
       "      <td>767</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_complex_words  gunning_fog_index  coleman_liau  dale_chall  \\\n",
       "0                   12          13.958368     16.042899   13.194432   \n",
       "1                   11          11.743640     12.374202   10.910638   \n",
       "2                   14          12.725641     12.238621   11.165187   \n",
       "3                   16          12.945318     14.739058   12.977291   \n",
       "4                   16          15.092105     14.640398   12.863592   \n",
       "\n",
       "   flesh_index     spache       ttr      lttr      grttr      cttr        dui  \\\n",
       "0    45.187189  25.627640  0.647343  0.894344   9.313644  0.459109  65.392650   \n",
       "1    66.336336  36.455923  0.665584  0.898881  11.680959  0.472046  80.655529   \n",
       "2    55.850096  47.448641  0.529557  0.865746  10.670271  0.375572  56.749126   \n",
       "3    47.256816  45.093900  0.568063  0.878406  11.102686  0.402881  62.504991   \n",
       "4    39.916678  54.833158  0.602386  0.895130  13.510102  0.427224  76.344441   \n",
       "\n",
       "         si      yulesk      yulesi  simpsonsd    herdans  mmf_clusters  \\\n",
       "0  0.949179   87.370562  114.455026   1.985697   8.409001             0   \n",
       "1  0.957786   92.069940  108.613082   1.988271   9.801463             0   \n",
       "2  0.937600  228.411962   43.780544   1.975126   8.828978             3   \n",
       "3  0.943930  123.017954   81.288947   1.986088   9.513030             0   \n",
       "4  0.953502  102.403620   97.652798   1.987356  11.476597             3   \n",
       "\n",
       "   words  upper  lower  sentences  syllables  special_characters  \n",
       "0    201     47    154          8        318                  35  \n",
       "1    299     50    249         14        414                  40  \n",
       "2    397     70    327         16        574                  61  \n",
       "3    377     51    326         15        590                  54  \n",
       "4    491     60    431         16        767                  74  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2ec63",
   "metadata": {},
   "source": [
    "### Implementing RF on the new Featured dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d4945c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=12),\n",
       "                   param_distributions={&#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=12),\n",
       "                   param_distributions={&#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=12)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=12)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=12),\n",
       "                   param_distributions={'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'n_estimators': array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])},\n",
       "                   scoring='accuracy')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(rf_model, param_grid, n_iter=10, cv=5,scoring='accuracy')\n",
    "random_search.fit(X_train_engineered, y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "524ed93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 150, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_depth': 1}\n",
      "Best Score: 0.5266900790166813\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "6345d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cHecking validation score\n",
    "rf_model = RandomForestClassifier(random_state=12,n_estimators=150, min_samples_split=9, min_samples_leaf=7, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "425db4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=1, min_samples_leaf=7, min_samples_split=9,\n",
       "                       n_estimators=150, random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=1, min_samples_leaf=7, min_samples_split=9,\n",
       "                       n_estimators=150, random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=1, min_samples_leaf=7, min_samples_split=9,\n",
       "                       n_estimators=150, random_state=12)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(X_train_engineered,y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "f3e2b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6041666666666666\n"
     ]
    }
   ],
   "source": [
    "## Train Score\n",
    "print(accuracy_score(y_train['target'],rf_model.predict(X_train_engineered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "a9954b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49411764705882355\n"
     ]
    }
   ],
   "source": [
    "## Test Score\n",
    "print(accuracy_score(y_test,rf_model.predict(X_test_engineered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "51bbb846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.64      0.15      0.24        47\n",
      "        real       0.46      0.89      0.61        38\n",
      "\n",
      "    accuracy                           0.48        85\n",
      "   macro avg       0.55      0.52      0.42        85\n",
      "weighted avg       0.56      0.48      0.40        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,rf_model.predict(X_test_engineered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2eeb9",
   "metadata": {},
   "source": [
    "### Implmementing xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b8739b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001]\n",
    "}\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1319146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(xgb, param_distributions=param_grid, n_iter=10, scoring=\"accuracy\", cv=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "be08eeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None, max_bin=None,...\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           max_leaves=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None, ...),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                                        &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "                   random_state=42, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None, max_bin=None,...\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           max_leaves=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None, ...),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                                        &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "                   random_state=42, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None, max_bin=None,...\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           max_leaves=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None, ...),\n",
       "                   param_distributions={'learning_rate': [0.1, 0.01, 0.001],\n",
       "                                        'max_depth': [3, 4, 5],\n",
       "                                        'n_estimators': [100, 200, 300]},\n",
       "                   random_state=42, scoring='accuracy')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_xgboost=[]\n",
    "for i in y_train['target']:\n",
    "    if i=='real':\n",
    "        y_train_xgboost.append(0)\n",
    "    else:\n",
    "        y_train_xgboost.append(1)\n",
    "random_search.fit(X_train_engineered, y_train_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "701fb374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.001}\n",
      "Best Score: 0.44925373134328356\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ed5acc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "43c6ff09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.001, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.001, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.001, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train_engineered,y_train_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6d44a82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6696428571428571\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train_xgboost,xgb.predict(X_train_engineered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bd33dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_xgboost=[]\n",
    "for i in y_test:\n",
    "    if i=='real':\n",
    "        y_test_xgboost.append(0)\n",
    "    else:\n",
    "        y_test_xgboost.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7bd7709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35294117647058826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.50      0.41        38\n",
      "           1       0.37      0.23      0.29        47\n",
      "\n",
      "    accuracy                           0.35        85\n",
      "   macro avg       0.36      0.37      0.35        85\n",
      "weighted avg       0.36      0.35      0.34        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test_xgboost,xgb.predict(X_test_engineered)))\n",
    "print(classification_report(y_test_xgboost,xgb.predict(X_test_engineered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705f12a",
   "metadata": {},
   "source": [
    "### Implemnting SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6eab805a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm=SVC()\n",
    "svm.fit(X_train_engineered, y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "538a559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5148809523809523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "print(accuracy_score(y_train['target'],svm.predict(X_train_engineered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "14fdaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [0.1, 0.5, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "44b87d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "grid_search = GridSearchCV(svm, param_grid, scoring=\"accuracy\", cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b3476b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;gamma&#x27;: [0.1, 0.5, 1],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;gamma&#x27;: [0.1, 0.5, 1],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10], 'gamma': [0.1, 0.5, 1],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train_engineered, y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "9a32bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best Score: 0.5148814749780509\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c25b5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4470588235294118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.00      0.00      0.00        47\n",
      "        real       0.45      1.00      0.62        38\n",
      "\n",
      "    accuracy                           0.45        85\n",
      "   macro avg       0.22      0.50      0.31        85\n",
      "weighted avg       0.20      0.45      0.28        85\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Madhurjya.Tamuly\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\Madhurjya.Tamuly\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\Madhurjya.Tamuly\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,svm.predict(X_test_engineered)))\n",
    "print(classification_report(y_test,svm.predict(X_test_engineered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf644eaa",
   "metadata": {},
   "source": [
    "### Creating NER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2d93afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "523f4910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 43.2 s\n",
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "person_ner_count=[]\n",
    "norp_ner_count=[]\n",
    "fac_ner_count=[]\n",
    "org_ner_count=[]\n",
    "gpe_ner_count=[]\n",
    "loc_ner_count=[]\n",
    "for each_sentence in X_train['Fake_Genuine']:\n",
    "    p=0\n",
    "    n=0\n",
    "    f=0\n",
    "    o=0\n",
    "    g=0\n",
    "    l=0\n",
    "    doc=nlp(each_sentence)\n",
    "    ner_tags = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    for each_tag in ner_tags:\n",
    "        if each_tag[1]=='PERSON':\n",
    "            p+=1\n",
    "        if each_tag[1]=='NORP':\n",
    "            n+=1\n",
    "        if each_tag[1]=='FAC':\n",
    "            f+=1\n",
    "        if each_tag[1]=='ORG':\n",
    "            o+=1\n",
    "        if each_tag[1]=='GPE':\n",
    "            g+=1\n",
    "        if each_tag[1]=='LOC':\n",
    "            l+=1    \n",
    "    person_ner_count.append(p)\n",
    "    norp_ner_count.append(n)\n",
    "    fac_ner_count.append(f)\n",
    "    org_ner_count.append(o)\n",
    "    gpe_ner_count.append(g)\n",
    "    loc_ner_count.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "99c1a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_engineered['person']=person_ner_count\n",
    "X_train_engineered['norp']=norp_ner_count\n",
    "X_train_engineered['fac']=fac_ner_count\n",
    "X_train_engineered['org']=org_ner_count\n",
    "X_train_engineered['gpe']=gpe_ner_count\n",
    "X_train_engineered['loc']=loc_ner_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ed1130f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.1 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "person_ner_count=[]\n",
    "norp_ner_count=[]\n",
    "fac_ner_count=[]\n",
    "org_ner_count=[]\n",
    "gpe_ner_count=[]\n",
    "loc_ner_count=[]\n",
    "for each_sentence in X_test['Fake_Genuine']:\n",
    "    p=0\n",
    "    n=0\n",
    "    f=0\n",
    "    o=0\n",
    "    g=0\n",
    "    l=0\n",
    "    doc=nlp(each_sentence)\n",
    "    ner_tags = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    for each_tag in ner_tags:\n",
    "        if each_tag[1]=='PERSON':\n",
    "            p+=1\n",
    "        if each_tag[1]=='NORP':\n",
    "            n+=1\n",
    "        if each_tag[1]=='FAC':\n",
    "            f+=1\n",
    "        if each_tag[1]=='ORG':\n",
    "            o+=1\n",
    "        if each_tag[1]=='GPE':\n",
    "            g+=1\n",
    "        if each_tag[1]=='LOC':\n",
    "            l+=1    \n",
    "    person_ner_count.append(p)\n",
    "    norp_ner_count.append(n)\n",
    "    fac_ner_count.append(f)\n",
    "    org_ner_count.append(o)\n",
    "    gpe_ner_count.append(g)\n",
    "    loc_ner_count.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6074f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_engineered['person']=person_ner_count\n",
    "X_test_engineered['norp']=norp_ner_count\n",
    "X_test_engineered['fac']=fac_ner_count\n",
    "X_test_engineered['org']=org_ner_count\n",
    "X_test_engineered['gpe']=gpe_ner_count\n",
    "X_test_engineered['loc']=loc_ner_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "558ee57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_complex_words</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>flesh_index</th>\n",
       "      <th>spache</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lttr</th>\n",
       "      <th>grttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>dui</th>\n",
       "      <th>si</th>\n",
       "      <th>yulesk</th>\n",
       "      <th>yulesi</th>\n",
       "      <th>simpsonsd</th>\n",
       "      <th>herdans</th>\n",
       "      <th>mmf_clusters</th>\n",
       "      <th>words</th>\n",
       "      <th>upper</th>\n",
       "      <th>lower</th>\n",
       "      <th>sentences</th>\n",
       "      <th>syllables</th>\n",
       "      <th>special_characters</th>\n",
       "      <th>person</th>\n",
       "      <th>norp</th>\n",
       "      <th>fac</th>\n",
       "      <th>org</th>\n",
       "      <th>gpe</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>10.827839</td>\n",
       "      <td>11.008814</td>\n",
       "      <td>9.651423</td>\n",
       "      <td>61.472051</td>\n",
       "      <td>50.936890</td>\n",
       "      <td>0.588983</td>\n",
       "      <td>0.895790</td>\n",
       "      <td>12.795987</td>\n",
       "      <td>0.417718</td>\n",
       "      <td>71.612027</td>\n",
       "      <td>0.950539</td>\n",
       "      <td>95.950145</td>\n",
       "      <td>104.220791</td>\n",
       "      <td>1.988191</td>\n",
       "      <td>11.237153</td>\n",
       "      <td>3</td>\n",
       "      <td>460</td>\n",
       "      <td>51</td>\n",
       "      <td>409</td>\n",
       "      <td>21</td>\n",
       "      <td>643</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>15.682480</td>\n",
       "      <td>14.623481</td>\n",
       "      <td>12.002560</td>\n",
       "      <td>39.317823</td>\n",
       "      <td>32.517072</td>\n",
       "      <td>0.604096</td>\n",
       "      <td>0.902660</td>\n",
       "      <td>10.340450</td>\n",
       "      <td>0.428437</td>\n",
       "      <td>64.013683</td>\n",
       "      <td>0.946505</td>\n",
       "      <td>88.851867</td>\n",
       "      <td>112.546875</td>\n",
       "      <td>1.987864</td>\n",
       "      <td>9.686140</td>\n",
       "      <td>-1</td>\n",
       "      <td>286</td>\n",
       "      <td>49</td>\n",
       "      <td>237</td>\n",
       "      <td>9</td>\n",
       "      <td>448</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>10.795483</td>\n",
       "      <td>10.879403</td>\n",
       "      <td>11.212054</td>\n",
       "      <td>71.349718</td>\n",
       "      <td>63.229321</td>\n",
       "      <td>0.600746</td>\n",
       "      <td>0.884849</td>\n",
       "      <td>13.908282</td>\n",
       "      <td>0.426061</td>\n",
       "      <td>77.495466</td>\n",
       "      <td>0.953990</td>\n",
       "      <td>73.753946</td>\n",
       "      <td>135.585965</td>\n",
       "      <td>1.987768</td>\n",
       "      <td>11.300794</td>\n",
       "      <td>0</td>\n",
       "      <td>528</td>\n",
       "      <td>76</td>\n",
       "      <td>452</td>\n",
       "      <td>26</td>\n",
       "      <td>702</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>12.966712</td>\n",
       "      <td>11.955512</td>\n",
       "      <td>10.786522</td>\n",
       "      <td>55.993690</td>\n",
       "      <td>65.495157</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.876743</td>\n",
       "      <td>12.500406</td>\n",
       "      <td>0.357493</td>\n",
       "      <td>60.195948</td>\n",
       "      <td>0.939339</td>\n",
       "      <td>108.518351</td>\n",
       "      <td>92.150313</td>\n",
       "      <td>1.986678</td>\n",
       "      <td>11.054827</td>\n",
       "      <td>0</td>\n",
       "      <td>605</td>\n",
       "      <td>81</td>\n",
       "      <td>524</td>\n",
       "      <td>22</td>\n",
       "      <td>857</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>8.020596</td>\n",
       "      <td>11.693594</td>\n",
       "      <td>10.947900</td>\n",
       "      <td>77.559871</td>\n",
       "      <td>63.766915</td>\n",
       "      <td>0.650391</td>\n",
       "      <td>0.883435</td>\n",
       "      <td>14.716660</td>\n",
       "      <td>0.461270</td>\n",
       "      <td>90.465621</td>\n",
       "      <td>0.960971</td>\n",
       "      <td>63.219625</td>\n",
       "      <td>158.178731</td>\n",
       "      <td>1.987387</td>\n",
       "      <td>10.932163</td>\n",
       "      <td>1</td>\n",
       "      <td>503</td>\n",
       "      <td>98</td>\n",
       "      <td>405</td>\n",
       "      <td>36</td>\n",
       "      <td>686</td>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_complex_words  gunning_fog_index  coleman_liau  dale_chall  \\\n",
       "0                   12          10.827839     11.008814    9.651423   \n",
       "1                   14          15.682480     14.623481   12.002560   \n",
       "2                   16          10.795483     10.879403   11.212054   \n",
       "3                   17          12.966712     11.955512   10.786522   \n",
       "4                   11           8.020596     11.693594   10.947900   \n",
       "\n",
       "   flesh_index     spache       ttr      lttr      grttr      cttr        dui  \\\n",
       "0    61.472051  50.936890  0.588983  0.895790  12.795987  0.417718  71.612027   \n",
       "1    39.317823  32.517072  0.604096  0.902660  10.340450  0.428437  64.013683   \n",
       "2    71.349718  63.229321  0.600746  0.884849  13.908282  0.426061  77.495466   \n",
       "3    55.993690  65.495157  0.504065  0.876743  12.500406  0.357493  60.195948   \n",
       "4    77.559871  63.766915  0.650391  0.883435  14.716660  0.461270  90.465621   \n",
       "\n",
       "         si      yulesk      yulesi  simpsonsd    herdans  mmf_clusters  \\\n",
       "0  0.950539   95.950145  104.220791   1.988191  11.237153             3   \n",
       "1  0.946505   88.851867  112.546875   1.987864   9.686140            -1   \n",
       "2  0.953990   73.753946  135.585965   1.987768  11.300794             0   \n",
       "3  0.939339  108.518351   92.150313   1.986678  11.054827             0   \n",
       "4  0.960971   63.219625  158.178731   1.987387  10.932163             1   \n",
       "\n",
       "   words  upper  lower  sentences  syllables  special_characters  person  \\\n",
       "0    460     51    409         21        643                  54       7   \n",
       "1    286     49    237          9        448                  34      11   \n",
       "2    528     76    452         26        702                  80      17   \n",
       "3    605     81    524         22        857                  54      21   \n",
       "4    503     98    405         36        686                  91       9   \n",
       "\n",
       "   norp  fac  org  gpe  loc  \n",
       "0     5    0    5    7    0  \n",
       "1     3    0    5    9    2  \n",
       "2     7    0    6    2    0  \n",
       "3     2    0    4    5    1  \n",
       "4     6    0    3    5    0  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_engineered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "29f6a30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_complex_words</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>flesh_index</th>\n",
       "      <th>spache</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lttr</th>\n",
       "      <th>grttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>dui</th>\n",
       "      <th>si</th>\n",
       "      <th>yulesk</th>\n",
       "      <th>yulesi</th>\n",
       "      <th>simpsonsd</th>\n",
       "      <th>herdans</th>\n",
       "      <th>mmf_clusters</th>\n",
       "      <th>words</th>\n",
       "      <th>upper</th>\n",
       "      <th>lower</th>\n",
       "      <th>sentences</th>\n",
       "      <th>syllables</th>\n",
       "      <th>special_characters</th>\n",
       "      <th>person</th>\n",
       "      <th>norp</th>\n",
       "      <th>fac</th>\n",
       "      <th>org</th>\n",
       "      <th>gpe</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13.958368</td>\n",
       "      <td>16.042899</td>\n",
       "      <td>13.194432</td>\n",
       "      <td>45.187189</td>\n",
       "      <td>25.627640</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.894344</td>\n",
       "      <td>9.313644</td>\n",
       "      <td>0.459109</td>\n",
       "      <td>65.392650</td>\n",
       "      <td>0.949179</td>\n",
       "      <td>87.370562</td>\n",
       "      <td>114.455026</td>\n",
       "      <td>1.985697</td>\n",
       "      <td>8.409001</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>47</td>\n",
       "      <td>154</td>\n",
       "      <td>8</td>\n",
       "      <td>318</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11.743640</td>\n",
       "      <td>12.374202</td>\n",
       "      <td>10.910638</td>\n",
       "      <td>66.336336</td>\n",
       "      <td>36.455923</td>\n",
       "      <td>0.665584</td>\n",
       "      <td>0.898881</td>\n",
       "      <td>11.680959</td>\n",
       "      <td>0.472046</td>\n",
       "      <td>80.655529</td>\n",
       "      <td>0.957786</td>\n",
       "      <td>92.069940</td>\n",
       "      <td>108.613082</td>\n",
       "      <td>1.988271</td>\n",
       "      <td>9.801463</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>50</td>\n",
       "      <td>249</td>\n",
       "      <td>14</td>\n",
       "      <td>414</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>12.725641</td>\n",
       "      <td>12.238621</td>\n",
       "      <td>11.165187</td>\n",
       "      <td>55.850096</td>\n",
       "      <td>47.448641</td>\n",
       "      <td>0.529557</td>\n",
       "      <td>0.865746</td>\n",
       "      <td>10.670271</td>\n",
       "      <td>0.375572</td>\n",
       "      <td>56.749126</td>\n",
       "      <td>0.937600</td>\n",
       "      <td>228.411962</td>\n",
       "      <td>43.780544</td>\n",
       "      <td>1.975126</td>\n",
       "      <td>8.828978</td>\n",
       "      <td>3</td>\n",
       "      <td>397</td>\n",
       "      <td>70</td>\n",
       "      <td>327</td>\n",
       "      <td>16</td>\n",
       "      <td>574</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>12.945318</td>\n",
       "      <td>14.739058</td>\n",
       "      <td>12.977291</td>\n",
       "      <td>47.256816</td>\n",
       "      <td>45.093900</td>\n",
       "      <td>0.568063</td>\n",
       "      <td>0.878406</td>\n",
       "      <td>11.102686</td>\n",
       "      <td>0.402881</td>\n",
       "      <td>62.504991</td>\n",
       "      <td>0.943930</td>\n",
       "      <td>123.017954</td>\n",
       "      <td>81.288947</td>\n",
       "      <td>1.986088</td>\n",
       "      <td>9.513030</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>51</td>\n",
       "      <td>326</td>\n",
       "      <td>15</td>\n",
       "      <td>590</td>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>15.092105</td>\n",
       "      <td>14.640398</td>\n",
       "      <td>12.863592</td>\n",
       "      <td>39.916678</td>\n",
       "      <td>54.833158</td>\n",
       "      <td>0.602386</td>\n",
       "      <td>0.895130</td>\n",
       "      <td>13.510102</td>\n",
       "      <td>0.427224</td>\n",
       "      <td>76.344441</td>\n",
       "      <td>0.953502</td>\n",
       "      <td>102.403620</td>\n",
       "      <td>97.652798</td>\n",
       "      <td>1.987356</td>\n",
       "      <td>11.476597</td>\n",
       "      <td>3</td>\n",
       "      <td>491</td>\n",
       "      <td>60</td>\n",
       "      <td>431</td>\n",
       "      <td>16</td>\n",
       "      <td>767</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_complex_words  gunning_fog_index  coleman_liau  dale_chall  \\\n",
       "0                   12          13.958368     16.042899   13.194432   \n",
       "1                   11          11.743640     12.374202   10.910638   \n",
       "2                   14          12.725641     12.238621   11.165187   \n",
       "3                   16          12.945318     14.739058   12.977291   \n",
       "4                   16          15.092105     14.640398   12.863592   \n",
       "\n",
       "   flesh_index     spache       ttr      lttr      grttr      cttr        dui  \\\n",
       "0    45.187189  25.627640  0.647343  0.894344   9.313644  0.459109  65.392650   \n",
       "1    66.336336  36.455923  0.665584  0.898881  11.680959  0.472046  80.655529   \n",
       "2    55.850096  47.448641  0.529557  0.865746  10.670271  0.375572  56.749126   \n",
       "3    47.256816  45.093900  0.568063  0.878406  11.102686  0.402881  62.504991   \n",
       "4    39.916678  54.833158  0.602386  0.895130  13.510102  0.427224  76.344441   \n",
       "\n",
       "         si      yulesk      yulesi  simpsonsd    herdans  mmf_clusters  \\\n",
       "0  0.949179   87.370562  114.455026   1.985697   8.409001             0   \n",
       "1  0.957786   92.069940  108.613082   1.988271   9.801463             0   \n",
       "2  0.937600  228.411962   43.780544   1.975126   8.828978             3   \n",
       "3  0.943930  123.017954   81.288947   1.986088   9.513030             0   \n",
       "4  0.953502  102.403620   97.652798   1.987356  11.476597             3   \n",
       "\n",
       "   words  upper  lower  sentences  syllables  special_characters  person  \\\n",
       "0    201     47    154          8        318                  35      14   \n",
       "1    299     50    249         14        414                  40      16   \n",
       "2    397     70    327         16        574                  61       3   \n",
       "3    377     51    326         15        590                  54       5   \n",
       "4    491     60    431         16        767                  74      10   \n",
       "\n",
       "   norp  fac  org  gpe  loc  \n",
       "0     2    0    6    2    0  \n",
       "1     1    0    5    5    1  \n",
       "2     2    0   21    4    1  \n",
       "3     0    0   12    0    0  \n",
       "4     1    0   10    7    1  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b493ba",
   "metadata": {},
   "source": [
    "### Creating BERT Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "b28efd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "model_name = 'sentence-transformers/distilbert-base-nli-mean-tokens'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "b59fa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=X_train['preprocessed_fake_genuine'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "1bcd3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d9fac060",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating test_embeddings\n",
    "text_test=X_test['preprocessed_fake_genuine_test'].tolist()\n",
    "encoded_input_test = tokenizer(text_test, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "81d2df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "0b76c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array(model_output[0])\n",
    "\n",
    "# Specify the file path to save the embeddings\n",
    "save_path = 'D:\\Downloads\\sentence_embeddings.npy'\n",
    "\n",
    "# Save the embeddings to the file\n",
    "np.save(save_path, embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "4e4342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output_test = model(**encoded_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "1c7bbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array_test = np.array(model_output_test[0])\n",
    "\n",
    "# Specify the file path to save the embeddings\n",
    "save_path = 'D:\\Downloads\\sentence_embeddings_test.npy'\n",
    "\n",
    "# Save the embeddings to the file\n",
    "np.save(save_path, embeddings_array_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1dfdfb",
   "metadata": {},
   "source": [
    "### Loading bert embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "52adb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array=np.load(r'D:\\Downloads\\sentence_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c64ed0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 393216)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(embeddings_array, (336, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f99b6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array_test=np.load(r'D:\\Downloads\\sentence_embeddings_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239907b",
   "metadata": {},
   "source": [
    "### Implementing RF on the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0d570219",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(rf_model, param_grid, n_iter=10, cv=5)\n",
    "random_search.fit(np.reshape(embeddings_array, (336, -1)), y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "de8bfd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 180, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 1}\n",
      "Best Score: 0.47603160667251976\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "36ad437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=12,n_estimators=180, min_samples_split=5, min_samples_leaf=4, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f405ffab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=1, min_samples_leaf=4, min_samples_split=5,\n",
       "                       n_estimators=180, random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=1, min_samples_leaf=4, min_samples_split=5,\n",
       "                       n_estimators=180, random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=1, min_samples_leaf=4, min_samples_split=5,\n",
       "                       n_estimators=180, random_state=12)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(np.reshape(embeddings_array, (336, -1)), y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "369655bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7202380952380952\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train['target'].tolist(),rf_model.predict(np.reshape(embeddings_array, (336, -1)))))\n",
    "\n",
    "#print(classification_report(y_test,rf_model.predict(X_train_engineered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2bb2bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test.tolist(),rf_model.predict(np.reshape(embeddings_array_test, (85, -1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "69105702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6041666666666666\n",
      "0.49411764705882355\n"
     ]
    }
   ],
   "source": [
    "### Implementing RF on the combined Linguistic Features\n",
    "rf_model = RandomForestClassifier(random_state=12,n_estimators=150, min_samples_split=9, min_samples_leaf=7, max_depth=1)\n",
    "\n",
    "rf_model.fit(X_train_engineered,y_train['target'])\n",
    "\n",
    "## Train Score\n",
    "print(accuracy_score(y_train['target'],rf_model.predict(X_train_engineered)))\n",
    "\n",
    "## Test Score\n",
    "print(accuracy_score(y_test,rf_model.predict(X_test_engineered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e70e9",
   "metadata": {},
   "source": [
    "### Implementing Neural Networks on Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "fb068117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "30ce0f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffafbc55e0e043c8a7b257869e47fe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d993416151944c6ebd581c1a8b389686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "7baf46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "3612cabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 512, 768)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "b85f10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential([layers.Dense(512,activation='relu',input_shape=(768,)),\n",
    "                          layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "25b4bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=tf.keras.Sequential([layers.Dense(512,activation='relu',input_shape=(768,)),\n",
    "                             layers.Dense(512,activation='relu'),\n",
    "                             layers.Dense(512,activation='relu'),\n",
    "                             layers.Dense(1024,activation='relu'),\n",
    "                          layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b658a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_y=tf.convert_to_tensor(y_train_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "bed3fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_y_test=[]\n",
    "for i in y_test:\n",
    "    if i=='real':\n",
    "        bert_y_test.append(0)\n",
    "    else:\n",
    "        bert_y_test.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0ecbb63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_y_test=tf.convert_to_tensor(bert_y_test)\n",
    "len(bert_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "b09049b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='BinaryCrossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6f89a732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([336, 768])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(embeddings_array,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "cf6849fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6596 - accuracy: 0.5923\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6241 - accuracy: 0.6280\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6045 - accuracy: 0.6607\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6638 - accuracy: 0.6190\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6035 - accuracy: 0.6726\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5983 - accuracy: 0.6458\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6383 - accuracy: 0.6280\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6408 - accuracy: 0.6399\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6141 - accuracy: 0.6429\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6014 - accuracy: 0.6488\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5740 - accuracy: 0.6875\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.5944 - accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5678 - accuracy: 0.6935\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5714 - accuracy: 0.6756\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5786 - accuracy: 0.6637\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6042 - accuracy: 0.6339\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6546 - accuracy: 0.6250\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.5610 - accuracy: 0.6815\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5613 - accuracy: 0.6935\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5565 - accuracy: 0.7173\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5573 - accuracy: 0.7232\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6095 - accuracy: 0.6577\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5408 - accuracy: 0.7113\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.5369 - accuracy: 0.7232\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5877 - accuracy: 0.6905\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5698 - accuracy: 0.6756\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5254 - accuracy: 0.7024\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5263 - accuracy: 0.6994\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.5745 - accuracy: 0.6935\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6174 - accuracy: 0.6488\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5507 - accuracy: 0.7054\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5248 - accuracy: 0.7054\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5274 - accuracy: 0.6994\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.5110 - accuracy: 0.7202\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5274 - accuracy: 0.7113\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7321\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5184 - accuracy: 0.7262\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6208 - accuracy: 0.6518\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5350 - accuracy: 0.7113\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5061 - accuracy: 0.7381\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5988 - accuracy: 0.6399\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.5108 - accuracy: 0.7262\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5236 - accuracy: 0.7113\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5020 - accuracy: 0.7411\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6016 - accuracy: 0.6786\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5624 - accuracy: 0.7054\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5218 - accuracy: 0.7351\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 0.7470\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5281 - accuracy: 0.6905\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7440\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4910 - accuracy: 0.7470\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5536 - accuracy: 0.7024\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.7679\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4770 - accuracy: 0.7381\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4843 - accuracy: 0.7470\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4901 - accuracy: 0.7321\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4812 - accuracy: 0.7560\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5450 - accuracy: 0.6935\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5450 - accuracy: 0.6875\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4765 - accuracy: 0.7411\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4554 - accuracy: 0.7530\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5043 - accuracy: 0.7262\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4616 - accuracy: 0.7470\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4840 - accuracy: 0.7321\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4578 - accuracy: 0.7292\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4449 - accuracy: 0.7530\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4592 - accuracy: 0.7560\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4951 - accuracy: 0.7024\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.7173\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4750 - accuracy: 0.7411\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4630 - accuracy: 0.7679\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5622 - accuracy: 0.6726\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5621 - accuracy: 0.6875\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5317 - accuracy: 0.7083\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4840 - accuracy: 0.7292\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4589 - accuracy: 0.7738\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4636 - accuracy: 0.7381\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4774 - accuracy: 0.7470\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5101 - accuracy: 0.7381\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4776 - accuracy: 0.7589\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5667 - accuracy: 0.6964\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4377 - accuracy: 0.7589\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4374 - accuracy: 0.7560\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4381 - accuracy: 0.7470\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4296 - accuracy: 0.7500\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7411\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4705 - accuracy: 0.7500\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4992 - accuracy: 0.7202\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4359 - accuracy: 0.7381\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4365 - accuracy: 0.7798\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4733 - accuracy: 0.7411\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4714 - accuracy: 0.7321\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4165 - accuracy: 0.7530\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4214 - accuracy: 0.7530\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.4140 - accuracy: 0.7500\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4195 - accuracy: 0.7381\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4471 - accuracy: 0.7411\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4512 - accuracy: 0.7321\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4085 - accuracy: 0.7560\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4160 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bd1e6ddfd0>"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf.reduce_max(embeddings_array,axis=1),bert_y,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "206db959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 [==============================] - 1s 91ms/step - loss: 0.9055 - accuracy: 0.5149 - val_loss: 0.7136 - val_accuracy: 0.4471\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.7012 - accuracy: 0.4851 - val_loss: 0.7325 - val_accuracy: 0.4471\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6933 - accuracy: 0.5595 - val_loss: 0.7015 - val_accuracy: 0.4706\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6830 - accuracy: 0.5833 - val_loss: 0.7499 - val_accuracy: 0.5529\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.7269 - accuracy: 0.4881 - val_loss: 0.7254 - val_accuracy: 0.4471\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6941 - accuracy: 0.4970 - val_loss: 0.6904 - val_accuracy: 0.5176\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6804 - accuracy: 0.5685 - val_loss: 0.7139 - val_accuracy: 0.4471\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6877 - accuracy: 0.5149 - val_loss: 0.7163 - val_accuracy: 0.4588\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6692 - accuracy: 0.5744 - val_loss: 0.7132 - val_accuracy: 0.4941\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6571 - accuracy: 0.6190 - val_loss: 0.7431 - val_accuracy: 0.4941\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6490 - accuracy: 0.6310 - val_loss: 0.7671 - val_accuracy: 0.5059\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6605 - accuracy: 0.6042 - val_loss: 0.7140 - val_accuracy: 0.5176\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6205 - accuracy: 0.6488 - val_loss: 0.8130 - val_accuracy: 0.4941\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6500 - accuracy: 0.6339 - val_loss: 0.7628 - val_accuracy: 0.4824\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6338 - accuracy: 0.6250 - val_loss: 0.7764 - val_accuracy: 0.4941\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6510 - accuracy: 0.5863 - val_loss: 0.7078 - val_accuracy: 0.4824\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6393 - accuracy: 0.6458 - val_loss: 0.7090 - val_accuracy: 0.5059\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6266 - accuracy: 0.6667 - val_loss: 0.7278 - val_accuracy: 0.4588\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5994 - accuracy: 0.6637 - val_loss: 0.7759 - val_accuracy: 0.4588\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5987 - accuracy: 0.6429 - val_loss: 0.8868 - val_accuracy: 0.5176\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6148 - accuracy: 0.6548 - val_loss: 0.7799 - val_accuracy: 0.5059\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6074 - accuracy: 0.6607 - val_loss: 0.7463 - val_accuracy: 0.4824\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5899 - accuracy: 0.6310 - val_loss: 0.7728 - val_accuracy: 0.4706\n",
      "Epoch 24/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5613 - accuracy: 0.6786 - val_loss: 0.8978 - val_accuracy: 0.4824\n",
      "Epoch 25/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6087 - accuracy: 0.6845 - val_loss: 1.3170 - val_accuracy: 0.5176\n",
      "Epoch 26/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6700 - accuracy: 0.6399 - val_loss: 0.8251 - val_accuracy: 0.4471\n",
      "Epoch 27/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6782 - accuracy: 0.5774 - val_loss: 0.7058 - val_accuracy: 0.4824\n",
      "Epoch 28/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6520 - accuracy: 0.5923 - val_loss: 0.7259 - val_accuracy: 0.4588\n",
      "Epoch 29/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6178 - accuracy: 0.6845 - val_loss: 0.7823 - val_accuracy: 0.4824\n",
      "Epoch 30/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5827 - accuracy: 0.6845 - val_loss: 0.8769 - val_accuracy: 0.4941\n",
      "Epoch 31/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5988 - accuracy: 0.6756 - val_loss: 0.8249 - val_accuracy: 0.4706\n",
      "Epoch 32/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5674 - accuracy: 0.6875 - val_loss: 0.9031 - val_accuracy: 0.5412\n",
      "Epoch 33/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5639 - accuracy: 0.6667 - val_loss: 0.8863 - val_accuracy: 0.4706\n",
      "Epoch 34/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5484 - accuracy: 0.6935 - val_loss: 0.9327 - val_accuracy: 0.4471\n",
      "Epoch 35/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5606 - accuracy: 0.7083 - val_loss: 1.2636 - val_accuracy: 0.5059\n",
      "Epoch 36/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6777 - accuracy: 0.6250 - val_loss: 0.7825 - val_accuracy: 0.4118\n",
      "Epoch 37/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6167 - accuracy: 0.6190 - val_loss: 0.7444 - val_accuracy: 0.4118\n",
      "Epoch 38/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5932 - accuracy: 0.7083 - val_loss: 0.7831 - val_accuracy: 0.4118\n",
      "Epoch 39/150\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5714 - accuracy: 0.6845 - val_loss: 0.8829 - val_accuracy: 0.4706\n",
      "Epoch 40/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5337 - accuracy: 0.6994 - val_loss: 0.9795 - val_accuracy: 0.4353\n",
      "Epoch 41/150\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5156 - accuracy: 0.6935 - val_loss: 1.0154 - val_accuracy: 0.4235\n",
      "Epoch 42/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5210 - accuracy: 0.7113 - val_loss: 1.0673 - val_accuracy: 0.4353\n",
      "Epoch 43/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6015 - accuracy: 0.6845 - val_loss: 1.0594 - val_accuracy: 0.4941\n",
      "Epoch 44/150\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6235 - accuracy: 0.6667 - val_loss: 0.8220 - val_accuracy: 0.4941\n",
      "Epoch 45/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6125 - accuracy: 0.6190 - val_loss: 0.7615 - val_accuracy: 0.4706\n",
      "Epoch 46/150\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5803 - accuracy: 0.6875 - val_loss: 0.8737 - val_accuracy: 0.4706\n",
      "Epoch 47/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5546 - accuracy: 0.6905 - val_loss: 0.8705 - val_accuracy: 0.4824\n",
      "Epoch 48/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5457 - accuracy: 0.6756 - val_loss: 0.8886 - val_accuracy: 0.4588\n",
      "Epoch 49/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5433 - accuracy: 0.6905 - val_loss: 1.2038 - val_accuracy: 0.5176\n",
      "Epoch 50/150\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5689 - accuracy: 0.7083 - val_loss: 0.9589 - val_accuracy: 0.4941\n",
      "Epoch 51/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5735 - accuracy: 0.6726 - val_loss: 1.0306 - val_accuracy: 0.4588\n",
      "Epoch 52/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5619 - accuracy: 0.6935 - val_loss: 0.8642 - val_accuracy: 0.5059\n",
      "Epoch 53/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5538 - accuracy: 0.6815 - val_loss: 0.9905 - val_accuracy: 0.4118\n",
      "Epoch 54/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5301 - accuracy: 0.7143 - val_loss: 0.9488 - val_accuracy: 0.4588\n",
      "Epoch 55/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5040 - accuracy: 0.7232 - val_loss: 1.2809 - val_accuracy: 0.4235\n",
      "Epoch 56/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5480 - accuracy: 0.6964 - val_loss: 1.1398 - val_accuracy: 0.4235\n",
      "Epoch 57/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5027 - accuracy: 0.7083 - val_loss: 1.0263 - val_accuracy: 0.4471\n",
      "Epoch 58/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4828 - accuracy: 0.7292 - val_loss: 1.1432 - val_accuracy: 0.4118\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4791 - accuracy: 0.7143 - val_loss: 1.2185 - val_accuracy: 0.4118\n",
      "Epoch 60/150\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4585 - accuracy: 0.7530 - val_loss: 1.3378 - val_accuracy: 0.4000\n",
      "Epoch 61/150\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4510 - accuracy: 0.7500 - val_loss: 1.3932 - val_accuracy: 0.4118\n",
      "Epoch 62/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4356 - accuracy: 0.7411 - val_loss: 1.4885 - val_accuracy: 0.4118\n",
      "Epoch 63/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4480 - accuracy: 0.7738 - val_loss: 1.3625 - val_accuracy: 0.4118\n",
      "Epoch 64/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5182 - accuracy: 0.7530 - val_loss: 1.0563 - val_accuracy: 0.4588\n",
      "Epoch 65/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4895 - accuracy: 0.7321 - val_loss: 1.0456 - val_accuracy: 0.3647\n",
      "Epoch 66/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4852 - accuracy: 0.7500 - val_loss: 1.1900 - val_accuracy: 0.4000\n",
      "Epoch 67/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4596 - accuracy: 0.7500 - val_loss: 1.4151 - val_accuracy: 0.4824\n",
      "Epoch 68/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5153 - accuracy: 0.7262 - val_loss: 1.2435 - val_accuracy: 0.4235\n",
      "Epoch 69/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4507 - accuracy: 0.7619 - val_loss: 1.5775 - val_accuracy: 0.3647\n",
      "Epoch 70/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4513 - accuracy: 0.7589 - val_loss: 1.3430 - val_accuracy: 0.4353\n",
      "Epoch 71/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4294 - accuracy: 0.7589 - val_loss: 1.4383 - val_accuracy: 0.4118\n",
      "Epoch 72/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4250 - accuracy: 0.7411 - val_loss: 1.5496 - val_accuracy: 0.4118\n",
      "Epoch 73/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4317 - accuracy: 0.7649 - val_loss: 1.6058 - val_accuracy: 0.4235\n",
      "Epoch 74/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4420 - accuracy: 0.7470 - val_loss: 1.3977 - val_accuracy: 0.4235\n",
      "Epoch 75/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4459 - accuracy: 0.7530 - val_loss: 1.8188 - val_accuracy: 0.3529\n",
      "Epoch 76/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4716 - accuracy: 0.7411 - val_loss: 1.5150 - val_accuracy: 0.4706\n",
      "Epoch 77/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4762 - accuracy: 0.7292 - val_loss: 1.4156 - val_accuracy: 0.4000\n",
      "Epoch 78/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4454 - accuracy: 0.7321 - val_loss: 1.4920 - val_accuracy: 0.4000\n",
      "Epoch 79/150\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4238 - accuracy: 0.7500 - val_loss: 1.9188 - val_accuracy: 0.4471\n",
      "Epoch 80/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5064 - accuracy: 0.7619 - val_loss: 1.2700 - val_accuracy: 0.4235\n",
      "Epoch 81/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5905 - accuracy: 0.6429 - val_loss: 0.8186 - val_accuracy: 0.4471\n",
      "Epoch 82/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5482 - accuracy: 0.7054 - val_loss: 1.0037 - val_accuracy: 0.4824\n",
      "Epoch 83/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5369 - accuracy: 0.6875 - val_loss: 0.9254 - val_accuracy: 0.4118\n",
      "Epoch 84/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4663 - accuracy: 0.7470 - val_loss: 1.3308 - val_accuracy: 0.4000\n",
      "Epoch 85/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4364 - accuracy: 0.7470 - val_loss: 1.7744 - val_accuracy: 0.4000\n",
      "Epoch 86/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5506 - accuracy: 0.7143 - val_loss: 1.2910 - val_accuracy: 0.4824\n",
      "Epoch 87/150\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6146 - accuracy: 0.6458 - val_loss: 0.8299 - val_accuracy: 0.4471\n",
      "Epoch 88/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5402 - accuracy: 0.6905 - val_loss: 0.8627 - val_accuracy: 0.4118\n",
      "Epoch 89/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5460 - accuracy: 0.7589 - val_loss: 0.7859 - val_accuracy: 0.4235\n",
      "Epoch 90/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5223 - accuracy: 0.7500 - val_loss: 1.2561 - val_accuracy: 0.3882\n",
      "Epoch 91/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4870 - accuracy: 0.7530 - val_loss: 1.0660 - val_accuracy: 0.4824\n",
      "Epoch 92/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5363 - accuracy: 0.7143 - val_loss: 1.1128 - val_accuracy: 0.4824\n",
      "Epoch 93/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6573 - accuracy: 0.6280 - val_loss: 0.9279 - val_accuracy: 0.4706\n",
      "Epoch 94/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5272 - accuracy: 0.7232 - val_loss: 0.9934 - val_accuracy: 0.4471\n",
      "Epoch 95/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5017 - accuracy: 0.7589 - val_loss: 0.8214 - val_accuracy: 0.4235\n",
      "Epoch 96/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5066 - accuracy: 0.7589 - val_loss: 0.9878 - val_accuracy: 0.3882\n",
      "Epoch 97/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4621 - accuracy: 0.7560 - val_loss: 1.2476 - val_accuracy: 0.3882\n",
      "Epoch 98/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4476 - accuracy: 0.7619 - val_loss: 1.8890 - val_accuracy: 0.3882\n",
      "Epoch 99/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4537 - accuracy: 0.7470 - val_loss: 1.4968 - val_accuracy: 0.4471\n",
      "Epoch 100/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4402 - accuracy: 0.7530 - val_loss: 1.5883 - val_accuracy: 0.3412\n",
      "Epoch 101/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4534 - accuracy: 0.7619 - val_loss: 1.3434 - val_accuracy: 0.4353\n",
      "Epoch 102/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4212 - accuracy: 0.7649 - val_loss: 1.6420 - val_accuracy: 0.3529\n",
      "Epoch 103/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4049 - accuracy: 0.7560 - val_loss: 2.0314 - val_accuracy: 0.3765\n",
      "Epoch 104/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3974 - accuracy: 0.7589 - val_loss: 2.2052 - val_accuracy: 0.3529\n",
      "Epoch 105/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4117 - accuracy: 0.7530 - val_loss: 2.0195 - val_accuracy: 0.4000\n",
      "Epoch 106/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4099 - accuracy: 0.7470 - val_loss: 1.7111 - val_accuracy: 0.3294\n",
      "Epoch 107/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4221 - accuracy: 0.7470 - val_loss: 1.4840 - val_accuracy: 0.4588\n",
      "Epoch 108/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4132 - accuracy: 0.7530 - val_loss: 1.8224 - val_accuracy: 0.3529\n",
      "Epoch 109/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4025 - accuracy: 0.7589 - val_loss: 1.9860 - val_accuracy: 0.3882\n",
      "Epoch 110/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.3903 - accuracy: 0.7738 - val_loss: 2.2950 - val_accuracy: 0.4118\n",
      "Epoch 111/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.3836 - accuracy: 0.7738 - val_loss: 2.6577 - val_accuracy: 0.3294\n",
      "Epoch 112/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5492 - accuracy: 0.7351 - val_loss: 1.3155 - val_accuracy: 0.3412\n",
      "Epoch 113/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4969 - accuracy: 0.7202 - val_loss: 0.9701 - val_accuracy: 0.3294\n",
      "Epoch 114/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5043 - accuracy: 0.7768 - val_loss: 0.9652 - val_accuracy: 0.4000\n",
      "Epoch 115/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4513 - accuracy: 0.7768 - val_loss: 1.6336 - val_accuracy: 0.3176\n",
      "Epoch 116/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4270 - accuracy: 0.7589 - val_loss: 1.6852 - val_accuracy: 0.4353\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4060 - accuracy: 0.7649 - val_loss: 2.2482 - val_accuracy: 0.3529\n",
      "Epoch 118/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3969 - accuracy: 0.7679 - val_loss: 2.4342 - val_accuracy: 0.3412\n",
      "Epoch 119/150\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3898 - accuracy: 0.7679 - val_loss: 2.2252 - val_accuracy: 0.3765\n",
      "Epoch 120/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4244 - accuracy: 0.7619 - val_loss: 2.1548 - val_accuracy: 0.4471\n",
      "Epoch 121/150\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5044 - accuracy: 0.7381 - val_loss: 1.2613 - val_accuracy: 0.3765\n",
      "Epoch 122/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4839 - accuracy: 0.7083 - val_loss: 1.2370 - val_accuracy: 0.4588\n",
      "Epoch 123/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4653 - accuracy: 0.7411 - val_loss: 1.2485 - val_accuracy: 0.3529\n",
      "Epoch 124/150\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4449 - accuracy: 0.7292 - val_loss: 1.6123 - val_accuracy: 0.3765\n",
      "Epoch 125/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4112 - accuracy: 0.7560 - val_loss: 1.9251 - val_accuracy: 0.3412\n",
      "Epoch 126/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4255 - accuracy: 0.7321 - val_loss: 2.0164 - val_accuracy: 0.3176\n",
      "Epoch 127/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4421 - accuracy: 0.7381 - val_loss: 1.7317 - val_accuracy: 0.4000\n",
      "Epoch 128/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4154 - accuracy: 0.7500 - val_loss: 1.6240 - val_accuracy: 0.3647\n",
      "Epoch 129/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3719 - accuracy: 0.7679 - val_loss: 2.0667 - val_accuracy: 0.3765\n",
      "Epoch 130/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4019 - accuracy: 0.7798 - val_loss: 3.0101 - val_accuracy: 0.4824\n",
      "Epoch 131/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6590 - accuracy: 0.7232 - val_loss: 1.2290 - val_accuracy: 0.2941\n",
      "Epoch 132/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5724 - accuracy: 0.6726 - val_loss: 1.0686 - val_accuracy: 0.4706\n",
      "Epoch 133/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5813 - accuracy: 0.6905 - val_loss: 0.7564 - val_accuracy: 0.4235\n",
      "Epoch 134/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6213 - accuracy: 0.6101 - val_loss: 0.7733 - val_accuracy: 0.4118\n",
      "Epoch 135/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5868 - accuracy: 0.7024 - val_loss: 0.8167 - val_accuracy: 0.3882\n",
      "Epoch 136/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5390 - accuracy: 0.7530 - val_loss: 0.9567 - val_accuracy: 0.3647\n",
      "Epoch 137/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4881 - accuracy: 0.7440 - val_loss: 1.1284 - val_accuracy: 0.3765\n",
      "Epoch 138/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4694 - accuracy: 0.7589 - val_loss: 1.3941 - val_accuracy: 0.3176\n",
      "Epoch 139/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4260 - accuracy: 0.7530 - val_loss: 1.4489 - val_accuracy: 0.3765\n",
      "Epoch 140/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4134 - accuracy: 0.7649 - val_loss: 1.6253 - val_accuracy: 0.4000\n",
      "Epoch 141/150\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4565 - accuracy: 0.7679 - val_loss: 1.7163 - val_accuracy: 0.3647\n",
      "Epoch 142/150\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4645 - accuracy: 0.7589 - val_loss: 1.7134 - val_accuracy: 0.3412\n",
      "Epoch 143/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4553 - accuracy: 0.7649 - val_loss: 1.4135 - val_accuracy: 0.4118\n",
      "Epoch 144/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4092 - accuracy: 0.7649 - val_loss: 1.4170 - val_accuracy: 0.3176\n",
      "Epoch 145/150\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4038 - accuracy: 0.7768 - val_loss: 1.4453 - val_accuracy: 0.3765\n",
      "Epoch 146/150\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3990 - accuracy: 0.7589 - val_loss: 1.6954 - val_accuracy: 0.2941\n",
      "Epoch 147/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.3952 - accuracy: 0.7679 - val_loss: 1.9236 - val_accuracy: 0.3882\n",
      "Epoch 148/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4230 - accuracy: 0.7500 - val_loss: 1.9902 - val_accuracy: 0.3529\n",
      "Epoch 149/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3886 - accuracy: 0.7857 - val_loss: 2.0806 - val_accuracy: 0.3176\n",
      "Epoch 150/150\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4277 - accuracy: 0.7738 - val_loss: 1.7909 - val_accuracy: 0.3412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1dfbada60>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(optimizer='adam',loss='BinaryCrossentropy',metrics=['accuracy'])\n",
    "model_1.fit(tf.reduce_max(embeddings_array,axis=1),bert_y,validation_data=(tf.reduce_max(embeddings_array_test,axis=1),bert_y_test),batch_size=100,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "9ff79d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_bert_nn_prediction=[]\n",
    "for i in model_1.predict(tf.reduce_max(embeddings_array,axis=1)):\n",
    "    proba=float(i)\n",
    "    if proba>=0.5:\n",
    "        predicted_bert_nn_prediction.append(1)\n",
    "    else:\n",
    "        predicted_bert_nn_prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "54c4908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(bert_y,predicted_bert_nn_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d7a5a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n",
      "0.3411764705882353\n"
     ]
    }
   ],
   "source": [
    "### Test Score\n",
    "predicted_bert_nn_prediction=[]\n",
    "for i in model_1.predict(tf.reduce_max(embeddings_array_test,axis=1)):\n",
    "    proba=float(i)\n",
    "    if proba>=0.5:\n",
    "        predicted_bert_nn_prediction.append(1)\n",
    "    else:\n",
    "        predicted_bert_nn_prediction.append(0)\n",
    "print(accuracy_score(bert_y_test,predicted_bert_nn_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "24bd5f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.39      0.35        38\n",
      "           1       0.38      0.30      0.33        47\n",
      "\n",
      "    accuracy                           0.34        85\n",
      "   macro avg       0.35      0.35      0.34        85\n",
      "weighted avg       0.35      0.34      0.34        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(bert_y_test,predicted_bert_nn_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fe7fe",
   "metadata": {},
   "source": [
    "### Incorporeating featured dataset with the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "507f2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_tensor=tf.convert_to_tensor(np.asarray(X_train_engineered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "5dd287ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_tensor_test=tf.convert_to_tensor(np.asarray(X_test_engineered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b8880eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor=tf.reduce_max(embeddings_array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "01bce27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor_test=tf.reduce_max(embeddings_array_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "93164a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_tensor=tf.cast(engineered_tensor, dtype=tf.float32)\n",
    "embeddings_tensor=tf.cast(embeddings_tensor, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "d8d548a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_tensor_test=tf.cast(engineered_tensor_test, dtype=tf.float32)\n",
    "embeddings_tensor_test=tf.cast(embeddings_tensor_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0ae90e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor = tf.concat([engineered_tensor, embeddings_tensor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "f55268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor_test = tf.concat([engineered_tensor_test, embeddings_tensor_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "8d9a1642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([336, 797]), TensorShape([85, 797]))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_tensor.shape,concatenated_tensor_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "519e4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Metric to monitor (e.g., validation loss)\n",
    "    patience=40,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the weights of the best-performing model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "7750dbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 1s 164ms/step - loss: 12.8134 - accuracy: 0.5030 - val_loss: 17.3567 - val_accuracy: 0.5529\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 17.6049 - accuracy: 0.4673 - val_loss: 2.6983 - val_accuracy: 0.4471\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.0264 - accuracy: 0.5327 - val_loss: 7.7089 - val_accuracy: 0.5529\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 7.0451 - accuracy: 0.4911 - val_loss: 5.2647 - val_accuracy: 0.4471\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.0788 - accuracy: 0.5149 - val_loss: 2.1148 - val_accuracy: 0.4471\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 2.2076 - accuracy: 0.4911 - val_loss: 1.4871 - val_accuracy: 0.5529\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.4338 - accuracy: 0.5089 - val_loss: 1.6054 - val_accuracy: 0.4471\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.5440 - accuracy: 0.5149 - val_loss: 0.8679 - val_accuracy: 0.4471\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.8556 - accuracy: 0.5268 - val_loss: 1.2038 - val_accuracy: 0.5529\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.1911 - accuracy: 0.4881 - val_loss: 0.6830 - val_accuracy: 0.6235\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6990 - accuracy: 0.5387 - val_loss: 0.9523 - val_accuracy: 0.4588\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.9332 - accuracy: 0.5149 - val_loss: 0.7302 - val_accuracy: 0.4588\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7212 - accuracy: 0.5179 - val_loss: 0.7330 - val_accuracy: 0.5412\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.7333 - accuracy: 0.4911 - val_loss: 0.6909 - val_accuracy: 0.4471\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6942 - accuracy: 0.5298 - val_loss: 0.7143 - val_accuracy: 0.4471\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.7065 - accuracy: 0.5208 - val_loss: 0.6940 - val_accuracy: 0.6000\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6996 - accuracy: 0.5268 - val_loss: 0.6847 - val_accuracy: 0.5765\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6924 - accuracy: 0.5238 - val_loss: 0.6919 - val_accuracy: 0.4588\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6939 - accuracy: 0.5595 - val_loss: 0.6878 - val_accuracy: 0.5059\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6877 - accuracy: 0.5685 - val_loss: 0.6839 - val_accuracy: 0.5765\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6918 - accuracy: 0.5506 - val_loss: 0.6967 - val_accuracy: 0.5529\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.7037 - accuracy: 0.5327 - val_loss: 0.6803 - val_accuracy: 0.5765\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6796 - accuracy: 0.5536 - val_loss: 0.7130 - val_accuracy: 0.4588\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7011 - accuracy: 0.5446 - val_loss: 0.7047 - val_accuracy: 0.4941\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6895 - accuracy: 0.5446 - val_loss: 0.6996 - val_accuracy: 0.5647\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6953 - accuracy: 0.5357 - val_loss: 0.6929 - val_accuracy: 0.5647\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6809 - accuracy: 0.5565 - val_loss: 0.7085 - val_accuracy: 0.5176\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6927 - accuracy: 0.5625 - val_loss: 0.6925 - val_accuracy: 0.5529\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6895 - accuracy: 0.5685 - val_loss: 0.6704 - val_accuracy: 0.6118\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6908 - accuracy: 0.5268 - val_loss: 0.6824 - val_accuracy: 0.5882\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.7032 - accuracy: 0.5030 - val_loss: 0.6758 - val_accuracy: 0.5765\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6810 - accuracy: 0.5595 - val_loss: 0.6896 - val_accuracy: 0.5059\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6820 - accuracy: 0.5804 - val_loss: 0.7059 - val_accuracy: 0.4588\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6885 - accuracy: 0.5506 - val_loss: 0.6891 - val_accuracy: 0.5529\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6733 - accuracy: 0.5893 - val_loss: 0.7097 - val_accuracy: 0.5765\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.7015 - accuracy: 0.5119 - val_loss: 0.7018 - val_accuracy: 0.5647\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6943 - accuracy: 0.5268 - val_loss: 0.6861 - val_accuracy: 0.5647\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6825 - accuracy: 0.5714 - val_loss: 0.7149 - val_accuracy: 0.5059\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.7030 - accuracy: 0.5804 - val_loss: 0.7014 - val_accuracy: 0.5765\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6852 - accuracy: 0.5714 - val_loss: 0.6920 - val_accuracy: 0.5412\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6853 - accuracy: 0.5179 - val_loss: 0.7135 - val_accuracy: 0.5529\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6955 - accuracy: 0.5298 - val_loss: 0.7018 - val_accuracy: 0.5412\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6768 - accuracy: 0.5595 - val_loss: 0.7029 - val_accuracy: 0.4941\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6735 - accuracy: 0.5863 - val_loss: 0.6974 - val_accuracy: 0.5294\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6726 - accuracy: 0.5893 - val_loss: 0.6896 - val_accuracy: 0.5529\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6709 - accuracy: 0.5744 - val_loss: 0.7106 - val_accuracy: 0.5529\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6960 - accuracy: 0.5030 - val_loss: 0.6986 - val_accuracy: 0.5529\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6807 - accuracy: 0.5446 - val_loss: 0.7012 - val_accuracy: 0.5412\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6683 - accuracy: 0.5923 - val_loss: 0.7051 - val_accuracy: 0.5176\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6671 - accuracy: 0.6012 - val_loss: 0.6970 - val_accuracy: 0.5294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b22f61a850>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_combined=tf.keras.Sequential([layers.Dense(512,activation='relu',input_shape=(797,)),\n",
    "                          layers.Dense(512,activation='relu'),\n",
    "                          layers.Dense(1024,activation='relu'),\n",
    "                          layers.Dense(1024,activation='relu'),\n",
    "                          layers.Dense(512,activation='relu'),\n",
    "                          layers.Dense(256,activation='relu'),\n",
    "                          layers.Dense(1,activation='sigmoid')])\n",
    "model_combined.compile(optimizer='adam',loss='BinaryCrossentropy',metrics=['accuracy'])\n",
    "model_combined.fit(concatenated_tensor,bert_y,validation_data=(concatenated_tensor_test,bert_y_test),callbacks=[early_stopping_callback],batch_size=300,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "b32ddbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "best_model_weights = model_combined.get_weights()\n",
    "model_combined.set_weights(best_model_weights)\n",
    "predictions_concatenated_data = model_combined.predict(concatenated_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "03107a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_concatenated_data_train = model_combined.predict(concatenated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "ca5ffd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Data Score\n",
    "prediction_concatenated_train=[]\n",
    "predicted_proba_train=[]\n",
    "for i in predictions_concatenated_data_train:\n",
    "    probs=float(i)\n",
    "    predicted_proba_train.append(i)\n",
    "    if probs>0.50:\n",
    "        prediction_concatenated_train.append(1)\n",
    "    else:\n",
    "        prediction_concatenated_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "d22edc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5446428571428571\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(bert_y,prediction_concatenated_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "17bab4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test Data Score\n",
    "prediction_concatenated=[]\n",
    "predicted_proba=[]\n",
    "for i in predictions_concatenated_data:\n",
    "    probs=float(i)\n",
    "    predicted_proba.append(i)\n",
    "    if probs>0.50:\n",
    "        prediction_concatenated.append(1)\n",
    "    else:\n",
    "        prediction_concatenated.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "650fd53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6235294117647059\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(bert_y_test,prediction_concatenated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "5eb32087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.55      0.57        38\n",
      "           1       0.65      0.68      0.67        47\n",
      "\n",
      "    accuracy                           0.62        85\n",
      "   macro avg       0.62      0.62      0.62        85\n",
      "weighted avg       0.62      0.62      0.62        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(bert_y_test,prediction_concatenated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "dec40da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(bert_y_test,prediction_concatenated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b79037",
   "metadata": {},
   "source": [
    "### Using bert Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "0111158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import AdamW\n",
    "\n",
    "# Example training data\n",
    "train_texts = X_train['preprocessed_fake_genuine'].tolist()\n",
    "train_labels =[0 if i=='real' else 1 for i in y_train['target'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "bc13cbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # Replace with the desired BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the training texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "# Create torch tensors for input and labels\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "8b9822a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Madhurjya.Tamuly\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.7138\n",
      "Epoch 2 - Average Loss: 0.7023\n",
      "Epoch 3 - Average Loss: 0.6871\n",
      "Epoch 4 - Average Loss: 0.6877\n",
      "Epoch 5 - Average Loss: 0.6989\n",
      "Epoch 6 - Average Loss: 0.6666\n",
      "Epoch 7 - Average Loss: 0.6379\n",
      "Epoch 8 - Average Loss: 0.6320\n",
      "Epoch 9 - Average Loss: 0.5982\n",
      "Epoch 10 - Average Loss: 0.5727\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset and a data loader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs, train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=8)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "c680b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=X_test['preprocessed_fake_genuine_test'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "a8399a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(test_sample, add_special_tokens=True, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "61865b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples=X_test['preprocessed_fake_genuine_test'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "ef52180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r'D:\\Downloads\\bert_model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "c160b255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D:\\\\Downloads\\\\tokenizer\\\\tokenizer_config.json',\n",
       " 'D:\\\\Downloads\\\\tokenizer\\\\special_tokens_map.json',\n",
       " 'D:\\\\Downloads\\\\tokenizer\\\\vocab.txt',\n",
       " 'D:\\\\Downloads\\\\tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(r'D:\\Downloads\\tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "c206ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(X_test['preprocessed_fake_genuine_test'].iloc[3], add_special_tokens=True, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f909f",
   "metadata": {},
   "source": [
    "### Loading the BErt saved model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "44eed254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "955f8dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_path = r'D:\\Downloads\\bert_model_weights'  # Replace with the actual path to your saved BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(r'D:\\Downloads\\tokenizer')\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_path,from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "4fefe455",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(X_test_preprocessed['preprocessed_fake_genuine_test'].iloc[1], add_special_tokens=True, return_tensors='pt')\n",
    "(inputs.keys())\n",
    "import tensorflow as tf\n",
    "input_ids = tf.convert_to_tensor(inputs['input_ids'].numpy())\n",
    "token_type_ids = tf.convert_to_tensor(inputs['token_type_ids'].numpy())\n",
    "attention_mask = tf.convert_to_tensor(inputs['attention_mask'].numpy())\n",
    "#outputs = model(**inputs)\n",
    "#predicted_class = torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8c8bfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, attention_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "8bb991cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "logits = outputs.logits\n",
    "probabilities = tf.nn.softmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "07a1d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_index = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "6dee64c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "82ffb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes=[]\n",
    "predicted_probabbilties=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "6c868940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_test_preprocessed['preprocessed_fake_genuine_test'])):\n",
    "    inputs = tokenizer.encode_plus(X_test_preprocessed['preprocessed_fake_genuine_test'].iloc[i], add_special_tokens=True, return_tensors='pt',truncation=True)\n",
    "    (inputs.keys())\n",
    "    import tensorflow as tf\n",
    "    input_ids = tf.convert_to_tensor(inputs['input_ids'].numpy())\n",
    "    token_type_ids = tf.convert_to_tensor(inputs['token_type_ids'].numpy())\n",
    "    attention_mask = tf.convert_to_tensor(inputs['attention_mask'].numpy())\n",
    "    outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "    logits = outputs.logits\n",
    "    probabilities = tf.nn.softmax(logits, axis=1)\n",
    "    predicted_class_index = np.argmax(probabilities, axis=1)\n",
    "    predicted_classes.append(predicted_class_index)\n",
    "    predicted_probabbilties.append(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e292040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bert=[0 if i=='real' else 1 for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "dba1925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predicted_classes=[]\n",
    "for i in (predicted_classes):\n",
    "    bert_predicted_classes.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "1df0c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49411764705882355\n"
     ]
    }
   ],
   "source": [
    "## Test Accuracy\n",
    "from sklearn.metrics import *\n",
    "print(accuracy_score(y_test_bert,bert_predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "0c472936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.61      0.52        38\n",
      "           1       0.56      0.40      0.47        47\n",
      "\n",
      "    accuracy                           0.49        85\n",
      "   macro avg       0.50      0.50      0.49        85\n",
      "weighted avg       0.51      0.49      0.49        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_bert,bert_predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e75555d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes_train=[]\n",
    "predicted_probabbilties_train=[]\n",
    "for i in range(len(X_train_preprocessed['preprocessed_fake_genuine'])):\n",
    "    inputs = tokenizer.encode_plus(X_train_preprocessed['preprocessed_fake_genuine'].iloc[i], add_special_tokens=True, return_tensors='pt',truncation=True)\n",
    "    (inputs.keys())\n",
    "    import tensorflow as tf\n",
    "    input_ids = tf.convert_to_tensor(inputs['input_ids'].numpy())\n",
    "    token_type_ids = tf.convert_to_tensor(inputs['token_type_ids'].numpy())\n",
    "    attention_mask = tf.convert_to_tensor(inputs['attention_mask'].numpy())\n",
    "    outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "    logits = outputs.logits\n",
    "    probabilities = tf.nn.softmax(logits, axis=1)\n",
    "    predicted_class_index = np.argmax(probabilities, axis=1)\n",
    "    predicted_classes_train.append(predicted_class_index)\n",
    "    predicted_probabbilties_train.append(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a36cb937",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_labels=[0 if i=='real' else 1 for i in y_train['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "681b6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predicted_classes_train=[]\n",
    "for i in (predicted_classes_train):\n",
    "    bert_predicted_classes_train.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "36225a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7351190476190477\n"
     ]
    }
   ],
   "source": [
    "##Training Score\n",
    "print(accuracy_score(bert_train_labels,bert_predicted_classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "929a88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementing RF on the concatenated tensor of embeddings and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ba6dc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': np.arange(100,250,10),\n",
    "    'max_depth': np.arange(1, 10),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "7a7f81e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=RandomForestClassifier(max_depth=1,\n",
       "                                                    min_samples_leaf=4,\n",
       "                                                    min_samples_split=5,\n",
       "                                                    n_estimators=180,\n",
       "                                                    random_state=12),\n",
       "                   param_distributions={&#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=RandomForestClassifier(max_depth=1,\n",
       "                                                    min_samples_leaf=4,\n",
       "                                                    min_samples_split=5,\n",
       "                                                    n_estimators=180,\n",
       "                                                    random_state=12),\n",
       "                   param_distributions={&#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=1, min_samples_leaf=4, min_samples_split=5,\n",
       "                       n_estimators=180, random_state=12)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=1, min_samples_leaf=4, min_samples_split=5,\n",
       "                       n_estimators=180, random_state=12)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=RandomForestClassifier(max_depth=1,\n",
       "                                                    min_samples_leaf=4,\n",
       "                                                    min_samples_split=5,\n",
       "                                                    n_estimators=180,\n",
       "                                                    random_state=12),\n",
       "                   param_distributions={'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'n_estimators': array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220,\n",
       "       230, 240])})"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(rf_model, param_grid, n_iter=10, cv=5)\n",
    "random_search.fit(np.array(concatenated_tensor), np.array(bert_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "0a70d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 190, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_depth': 2}\n",
      "Best Score: 0.4314749780509219\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "b9c41f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=RandomForestClassifier(n_estimators=190, min_samples_split=6, min_samples_leaf= 6, max_depth=2,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "b16df227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=2, min_samples_leaf=6, min_samples_split=6,\n",
       "                       n_estimators=190, random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=2, min_samples_leaf=6, min_samples_split=6,\n",
       "                       n_estimators=190, random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=2, min_samples_leaf=6, min_samples_split=6,\n",
       "                       n_estimators=190, random_state=12)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(np.array(concatenated_tensor),np.array(bert_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "d1467416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7232142857142857\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(np.array(bert_y),rf_model.predict(np.array(concatenated_tensor))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "d958d048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(np.array(bert_y_test),rf_model.predict(np.array(concatenated_tensor_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532325da",
   "metadata": {},
   "source": [
    "### Combining the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f7780",
   "metadata": {},
   "source": [
    "### For linguistic approaches,RF is the best in terms of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "22331dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnlp_predictions=pd.read_csv(r'D:\\Downloads\\qnlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "17d269e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(accuracy_score(y_train['target'],rf_model.predict(X_train_engineered)))\n",
    "\n",
    "## Test Score\n",
    "#print(accuracy_score(y_test,rf_model.predict(X_test_engineered)))\n",
    "pipeline_df=pd.DataFrame()\n",
    "pipeline_df['Actual Test Labels']=y_test\n",
    "pipeline_df['engineered_proba']=rf_model.predict_proba(X_test_engineered).tolist()\n",
    "pipeline_df['engineered_predictions_rf']=rf_model.predict(X_test_engineered)\n",
    "pipeline_df['bert_embeddings_and_engineered_features_NN']=['fake' if i==1 else 'real' for i in prediction_concatenated]\n",
    "pipeline_df['bert_classifier']=['fake' if i==1 else 'real' for i in bert_predicted_classes]\n",
    "pipeline_df['qnlp_predictions']=qnlp_predictions['predictions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "28093832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Test Labels</th>\n",
       "      <th>engineered_proba</th>\n",
       "      <th>engineered_predictions_rf</th>\n",
       "      <th>bert_embeddings_and_engineered_features_NN</th>\n",
       "      <th>bert_classifier</th>\n",
       "      <th>qnlp_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>fake</td>\n",
       "      <td>[0.45541223358725136, 0.5445877664127485]</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>fake</td>\n",
       "      <td>[0.47514307701018016, 0.5248569229898198]</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>real</td>\n",
       "      <td>[0.45655465408765195, 0.5434453459123482]</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fake</td>\n",
       "      <td>[0.47592369524001793, 0.5240763047599822]</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>real</td>\n",
       "      <td>[0.48101333184901574, 0.5189866681509843]</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>real</td>\n",
       "      <td>[0.4564092802091031, 0.543590719790897]</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual Test Labels                           engineered_proba  \\\n",
       "184               fake  [0.45541223358725136, 0.5445877664127485]   \n",
       "33                fake  [0.47514307701018016, 0.5248569229898198]   \n",
       "380               real  [0.45655465408765195, 0.5434453459123482]   \n",
       "6                 fake  [0.47592369524001793, 0.5240763047599822]   \n",
       "95                real  [0.48101333184901574, 0.5189866681509843]   \n",
       "138               real    [0.4564092802091031, 0.543590719790897]   \n",
       "\n",
       "    engineered_predictions_rf bert_embeddings_and_engineered_features_NN  \\\n",
       "184                      real                                       real   \n",
       "33                       real                                       fake   \n",
       "380                      real                                       fake   \n",
       "6                        real                                       fake   \n",
       "95                       real                                       fake   \n",
       "138                      real                                       real   \n",
       "\n",
       "    bert_classifier qnlp_predictions  \n",
       "184            real             real  \n",
       "33             fake             real  \n",
       "380            fake             fake  \n",
       "6              fake             fake  \n",
       "95             fake             fake  \n",
       "138            fake             real  "
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "742bea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cerating Majority Voting\n",
    "majority_list=[]\n",
    "for i in range(len(pipeline_df)):\n",
    "    temp_list=[]\n",
    "    temp_list.append(pipeline_df['bert_embeddings_and_engineered_features_NN'].iloc[i])\n",
    "    temp_list.append(pipeline_df['bert_classifier'].iloc[i])\n",
    "    temp_list.append(pipeline_df['qnlp_predictions'].iloc[i])\n",
    "    majority_list.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "0c866cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voting_predictions=[]\n",
    "for i in majority_list:\n",
    "    max_occurrence = max(set(i), key=i.count)\n",
    "    majority_voting_predictions.append(max_occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "5c6ce724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.60      0.53      0.56        47\n",
      "        real       0.49      0.55      0.52        38\n",
      "\n",
      "    accuracy                           0.54        85\n",
      "   macro avg       0.54      0.54      0.54        85\n",
      "weighted avg       0.55      0.54      0.54        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,majority_voting_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "c5820e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6235294117647059\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(bert_y_test,prediction_concatenated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb558bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
